{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstractive-Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdavid033/Research-Paper-Summarization/blob/master/Abstractive_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBBwVrjm1Sy0",
        "colab_type": "text"
      },
      "source": [
        "# Research Paper Summarization\n",
        "\n",
        "### Team:\n",
        "Kevin Thomas <br>\n",
        "Janani Arunachalam <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5G5Vn1v3zLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tqdm version 4.36.1 is required\n",
        "\n",
        "\n",
        "!pip install tqdm==4.36.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFl-9dmyVLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounting Drive\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzNw1MpvybVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libaries\n",
        "\n",
        "\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re           \n",
        "import glob\n",
        "from bs4 import BeautifulSoup \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "import keras\n",
        "import warnings\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nFwP48g3w__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Progress bar\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjUTMTwe1WOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading Pickle'd file which has data stored in a dataframe\n",
        "# with headings: \"text\", \"filenames\", \"highlights\", \"body\"\n",
        "\n",
        "# orig\n",
        "# data = pd.read_pickle(\"/content/drive/My Drive/NLP Project/Project Final/papers.pkl\")\n",
        "\n",
        "data = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/papers.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7l1Rf1g1r3Z",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCxpWyVhyFHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing \"body\" text\n",
        "\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        " \n",
        "def clean_body(text):\n",
        "    newText = text.lower()\n",
        "    newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
        "    newText = ' '.join(newText.split())\n",
        "    tokens = [w for w in newText.split() if not w in STOP_WORDS]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "cleaned_body = []\n",
        "for t in data['body']:\n",
        "    cleaned_body.append(clean_body(t))\n",
        "\n",
        "cleaned_body[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5PGcyem1fYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing \"highlight\" text\n",
        "\n",
        "\n",
        "def clean_highlight(text):\n",
        "  newText = text.lower()\n",
        "  newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
        "  newText = ' '.join(newText.split())\n",
        "  newText = '_START_ '+ newText + ' _END_'\n",
        "  return newText\n",
        "\n",
        "cleaned_highlight = []\n",
        "for t in data['highlights']:\n",
        "    cleaned_highlight.append(clean_highlight(t))\n",
        "\n",
        "cleaned_highlight[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrCEGfX29OAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Storing preprocessed data in the dataframe\n",
        "\n",
        "\n",
        "data['cleaned_highlights'] = cleaned_highlight\n",
        "data['cleaned_body'] = cleaned_body"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoU_mCHouOx-",
        "colab_type": "text"
      },
      "source": [
        "Retriving 5000 papers from the dataframe as more computational power is needed for the entire 8k data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXzYQJsExMA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yso-fdsWxYDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q1XQiGXucxP",
        "colab_type": "text"
      },
      "source": [
        "Visualizing the document length distribution for \"body\" and \"highlights\" to estimate optimal maximum length to create equal length vectors for words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7LvNebUubSu",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "for i in data['cleaned_body']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_highlights']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'Body':text_word_count, 'Highlights':summary_word_count})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f93_mXH2_LXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_body = 1000\n",
        "max_len_highlight = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlkcLDYHmhAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting cleaned data into strings\n",
        "\n",
        "\n",
        "data.cleaned_body = data.cleaned_body.progress_apply(lambda x: str(x))\n",
        "data.cleaned_highlights = data.cleaned_highlights.progress_apply(lambda x: str(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9f-RGEriHLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting data into training and test sets\n",
        "# Test set is 20% of total data\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(data['cleaned_body'],data['cleaned_highlights'],test_size=0.2,random_state=0,shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2_3_c3vdZt",
        "colab_type": "text"
      },
      "source": [
        "Tokenizer class from Keras is used to create equal length vectors for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpHCxYUbjJPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing \"body\"\n",
        "x_tok = Tokenizer()\n",
        "x_tok.fit_on_texts(list(x_train))\n",
        "\n",
        "# Converting text to number sequences\n",
        "x_train = x_tok.texts_to_sequences(x_train) \n",
        "x_test = x_tok.texts_to_sequences(x_test)\n",
        "\n",
        "# Padding zero upto maximum length\n",
        "x_train = pad_sequences(x_train,  maxlen=max_len_body, padding='post') \n",
        "x_test = pad_sequences(x_test, maxlen=max_len_body, padding='post')\n",
        "\n",
        "# Total number of words\n",
        "x_vocab_size = len(x_tok.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F62DsNW8lIaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing \"highlights\"\n",
        "y_tok = Tokenizer()\n",
        "y_tok.fit_on_texts(list(y_train))\n",
        "\n",
        "# Converting text to number sequences\n",
        "y_train = y_tok.texts_to_sequences(y_train) \n",
        "y_test = y_tok.texts_to_sequences(y_test)\n",
        "\n",
        "# Padding zero upto maximum length\n",
        "y_train = pad_sequences(y_train,  maxlen=max_len_highlight, padding='post') \n",
        "y_test = pad_sequences(y_test, maxlen=max_len_highlight, padding='post')\n",
        "\n",
        "# Word count\n",
        "y_vocab_size = len(y_tok.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk7dNvoUsN_S",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93pUkCJsUge",
        "colab_type": "text"
      },
      "source": [
        "Attention Layer required to remember context information. As there is no Keras implementation for Attention Layer we have found a third-party resource to aid in our model. Reference: https://github.com/thushv89/attention_keras/blob/master/layers/attention.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZzO2tHAn4TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEnxdZFbsZJV",
        "colab_type": "text"
      },
      "source": [
        "Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMZwZzg1N7e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I2xjhSnN5Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gekeC5WIl4U5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras import backend as K \n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "K.clear_session() \n",
        "latent_dim = 50 \n",
        "\n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_len_body,)) \n",
        "enc_emb = Embedding(x_vocab_size, latent_dim,trainable=True)(encoder_inputs) \n",
        "\n",
        "# 1st LSTM Layer\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "# 2nd LSTM Layer\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "# 3rd LSTM Layer\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# Decoder \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(y_vocab_size, latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "# LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "# Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) \n",
        "\n",
        "# Model Definition\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBQpEFfwjmb",
        "colab_type": "text"
      },
      "source": [
        "RMSProp was found to be the best optimizer for text summarization and sparse categorical crossentropy was found to be the best loss functiom from this reference: https://hackernoon.com/text-summarization-using-keras-models-366b002408d9\n",
        "\n",
        "Sparse categorical cross entropy is used as the loss function because it automatically converts an integer sequence to a one-hot encoded vector, hence requiring less memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb4o-pcMnmR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL253riQxSi7",
        "colab_type": "text"
      },
      "source": [
        "Validation loss was monitored to make sure that the model does not get worse. The model training is stopped if validation loss for the current iteration is higher than the validation loss for the previous iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbPRkDhaJfvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-zxfc2Use98",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELKGfGSSJ-jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:], epochs=30, callbacks=[es], batch_size=128, validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fizr8YPC-QZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/NLP Project/Project Final/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogiyb96NK0zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing training ans test loss functions\n",
        "\n",
        "\n",
        "from matplotlib import pyplot \n",
        "pyplot.plot(history.history['loss'], label='train') \n",
        "pyplot.plot(history.history['val_loss'], label='test') \n",
        "pyplot.legend() \n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg_mIo16ytnB",
        "colab_type": "text"
      },
      "source": [
        "Building dictionary for target and source words, to convert the index to word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gEYRwn-A3i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tok.index_word \n",
        "reverse_source_word_index=x_tok.index_word \n",
        "target_word_index=y_tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys7Rrz2tsinX",
        "colab_type": "text"
      },
      "source": [
        "## Inference Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOhWVQw4APQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder Inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder Inference\n",
        "# Below tensors hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_body,latent_dim))\n",
        "\n",
        "# Getting decoder sequence embeddings\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Predicting the next word in the sequence\n",
        "# Setting the initial states to the previous time step states\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# Attention Inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# Dense softmax layer to calculate probability distribution over target vocab\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final Decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QEq35cBAPIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to implement inference\n",
        "\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encoding input as state vectors\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generating empty target sequence of length 1\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Taking the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        try:\n",
        "            sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        except:\n",
        "            sampled_token = reverse_target_word_index[np.random.randint(1, len(reverse_target_word_index))]\n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_highlight-1)):\n",
        "                stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pTxOJBc0lb2",
        "colab_type": "text"
      },
      "source": [
        "Function to convert interger sequence to word sequence for highlights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu6LNrvFAO8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2highlights(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RscqnZlD1BoC",
        "colab_type": "text"
      },
      "source": [
        "## Displaying Results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFwhBovZAjic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reference = []\n",
        "hypothesis = []\n",
        "for i in range(10):\n",
        "  print(\"Highlights:\")\n",
        "  print(seq2summary(y_test[i]))\n",
        "  reference.append(seq2highlights(y_test[i]))\n",
        "  print(\"\\n\")\n",
        "  print(\"Predicted summary:\")\n",
        "  print(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
        "  hypothesis.append(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
        "  print(\"\\n\")\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZktzSNvkjPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np9ldkA5tX_7",
        "colab_type": "text"
      },
      "source": [
        "## Calculating Rouge Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa_-zaknr3C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXfbHFbhqzvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = Rouge()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCtxBD7rqzsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score.get_scores(hypothesis, reference, avg = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jeQkspBrJhy",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "ROUGH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBF76h7drbUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading papers, cleaning, creating & pickling dataframe\n",
        "\n",
        "\n",
        "def read_paper(path):\n",
        "  \"\"\"\n",
        "  Reads research papers and store them in a string\n",
        "  \"\"\"\n",
        "  f = open(path, 'r', encoding=\"utf-8\")\n",
        "  text = str(f.read())\n",
        "  f.close()\n",
        "  return text\n",
        "\n",
        "\n",
        "def create_dataframe(NO_INPUT_PAPERS):\n",
        "  \"\"\"\n",
        "  Takes number of papers to read and stores data in a dataframe\n",
        "  \"\"\"\n",
        "  temp_papers = []\n",
        "  filenames = []\n",
        "  for filename in tqdm(glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")[:NO_INPUT_PAPERS]):\n",
        "      temp_papers.append(read_paper(filename))\n",
        "      filenames.append(filename)\n",
        "  return pd.DataFrame(list(zip(temp_papers, filenames)), columns =['text', 'filenames'])\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "  \"\"\"\n",
        "  Removes unwanted characters, accounting for unicode characters\n",
        "  \"\"\"\n",
        "  text = re.sub(\"@&#\", \" \", text)\n",
        "  text = re.sub(\"\\n\", \" \", text)\n",
        "  text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "  return text\n",
        "\n",
        "\n",
        "data = create_dataframe(NO_INPUT_PAPERS = 10000)\n",
        "\n",
        "data['highlights'] = data['text'].progress_apply(lambda x: re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', x,  flags = re.I)[0])\n",
        "data = data[data.highlights != '    ']\n",
        "\n",
        "data['body'] = data['text'].progress_apply(lambda x: re.findall(r'.*(?:abstract )(.*?)$', x,  flags = re.I)[0])\n",
        "\n",
        "data.to_pickle(\"./papers.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-upXdNQPGPh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_pickle(\"/content/drive/My Drive/NLP Project/Project Final/data.pkl\")\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eth5gY49rJOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to find filename which don't have \"highlights\" in it\n",
        "\n",
        "\n",
        "def file_name(NO_INPUT_PAPERS):\n",
        "  temp_papers = []\n",
        "  for filename in glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")[:NO_INPUT_PAPERS]:\n",
        "    text = read_paper(filename)\n",
        "    text = re.sub(\"@&#\", \" \", text)\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)\n",
        "    if highlights == []:\n",
        "      print(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYkIYLB7rIkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}