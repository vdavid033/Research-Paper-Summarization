{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstractive-Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdavid033/Research-Paper-Summarization/blob/master/Abstractive_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBBwVrjm1Sy0",
        "colab_type": "text"
      },
      "source": [
        "# Research Paper Summarization\n",
        "\n",
        "### Team:\n",
        "Kevin Thomas <br>\n",
        "Janani Arunachalam <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5G5Vn1v3zLS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab64982d-e13d-458c-a6a0-8e04aace51ef"
      },
      "source": [
        "# tqdm version 4.36.1 is required\n",
        "\n",
        "\n",
        "!pip install tqdm==4.36.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm==4.36.1 in /usr/local/lib/python3.6/dist-packages (4.36.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFl-9dmyVLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e02e62e0-1b1d-4ccc-c35d-2dd777ac3ec3"
      },
      "source": [
        "# Mounting Drive\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzNw1MpvybVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e0fd75bc-5716-4886-93fa-014a74f98d95"
      },
      "source": [
        "# Importing Libaries\n",
        "\n",
        "\n",
        "import numpy as np  \n",
        "import pandas as pd \n",
        "import re           \n",
        "import glob\n",
        "from bs4 import BeautifulSoup \n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords   \n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "import keras\n",
        "import warnings\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nFwP48g3w__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Progress bar\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjUTMTwe1WOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading Pickle'd file which has data stored in a dataframe\n",
        "# with headings: \"text\", \"filenames\", \"highlights\", \"body\"\n",
        "\n",
        "# orig\n",
        "# data = pd.read_pickle(\"/content/drive/My Drive/NLP Project/Project Final/papers.pkl\")\n",
        "\n",
        "data = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/papers.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7l1Rf1g1r3Z",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCxpWyVhyFHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "8b143b2e-aed6-45b1-ea4d-b963990b64dc"
      },
      "source": [
        "# Preprocessing \"body\" text\n",
        "\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        " \n",
        "def clean_body(text):\n",
        "    newText = text.lower()\n",
        "    newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
        "    newText = ' '.join(newText.split())\n",
        "    tokens = [w for w in newText.split() if not w in STOP_WORDS]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "cleaned_body = []\n",
        "for t in data['body']:\n",
        "    cleaned_body.append(clean_body(t))\n",
        "\n",
        "cleaned_body[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['deterministic approaches simultaneously solve different interrelated optimisation problems lead general class nonlinear complementarity problem ncp. differentiability convexity requirements problems sophisticated algorithms introduced literature. paper develops evolutionary algorithm solve ncps. proposed approach parallel search multiple populations representing different agents evolve simultaneously whilst contact other. context agent autonomously solves optimisation programme sharing decisions neighbouring agents affects actions. framework applied environmental aerospace application obtained results compared found literature. convergence scalability approach tested search algorithm performance analysed. results encourage application evolutionary based algorithm complementarity problems future work investigate development performance improvements. study class problems solutions interrelated optimisation problems simultaneously required. context agent solves optimisation problem seeks optimal strategies interacting others. agents problem formed find best response decision interacting agents. precisely given function representing agents find simultaneously solving following optimisation problems maximise subject agent controls vector optimise objective function subject constraints set containing interrelation explained objective function constraints depend agents decisions explains solution agents problem affects others. representation model agents personal interests. given aggregated optimisation framework seeks collective optimality necessarily comply agents interest solve aforementioned problem deterministic approaches simultaneously solve problems lead system nonlinear equations formed agents karushkuhntucker kkt formulation known conditions kkt satisfies necessary sufficient condition optimality problem employing kkt depends properties function smooth continuous function. deterministic techniques generally exercise pivoting algorithm interior point based approach generalised newton method find feasible solution e.g. look 23. complementarity conditions introduced kkt provide insight modelling techniques leading general class nonlinear complementarity problems ncp economics traffic modelling robotics fluid dynamics energy planning nondifferentiability nonconvexity function deterministic approaches efficient. potential multimodality requires good starting point ensure convergence. avoid issues application global gradientfree approaches encouraged solve resulting ncps. example works decomposition based method coupled genetic algorithm orthogonal genetic algorithm successive reformulated optimisation problem ncp constraint satisfaction evolutionary based approach nikaido isoda function mapping 11. general agent based approaches presented literature solve complex interrelated optimisation problems. ouelhadj petrovic report application agent based modelling complex scheduling dynamic environments. cowling al. lau al. simulate supply chain system multiple independent autonomous agents contractor selection. sauvageau frayret use agent approaches represent paper pulp recycling supply chain. transportation scheduling planning agent based approaches successfully employed optimisation train coupling systems 1617 routing decision making based local information dynamic environment solving dynamic scheduling problem distributed project selfinterested participants dealing energy systems planning forecasting land usage environmental planning 21. combination operational research agent modelling implemented developing decision support system supply chain coordination context 2224. paper present alternative approach solve problems directly employing kkt ncp formulations. employ evolutionary algorithm solves interrelated optimisation problems parallel. section explains sharing strategies section develops algorithm gives demonstration illustrative example. section applies proposed approach problems environmental water pollution fluid dynamics discusses results. scalability method higher dimension search algorithm performance investigated section section concludes paper extra discussion future work. multiple agents communicate decision problems interrelated. fact agent deals problem reacts decisions environment agent finds best response decisions others. given propose solve interrelated agent problems idea parallel genetic algorithm 25. parallel combines hardware speed parallel processors software speed intelligent parallel searching 26. idea parallel divide problems search population multiple processors typically performance reasons determines multiple subpopulations information exchange 2728. implementation promises substantial performance achievement leads extensive attempts design improve competitive distributed hardware effective population communication migration fitness evaluations speeding 2931. algorithms literature run identical parallel run processor differ linkage populations information sharing e.g. look 3234. paper instead dividing population multiple processors borrow idea dedicate agent population solve problem interacting others. formally define agent problem let vector containing decision variables agent problem let vector containing decision variables agents involved problem excluding agent define problem formed use parallel idea coevolution solve extension subproblem objective function. interconnection agents problems solve problem whilst communicates problems sharing information. concept gain faster convergence pareto solution multiobjective optimisation problem. formally search algorithm described different search trajectories performing parallel following linkage shows interconnection agents. acts synchronisation linkage agent optimise problem given decisions interacting agents neighbourhood remain fixed shown describes value updated search problem generation linking decisions fact problem reaction agent agents decisions given problem agent knows problem components communicating neighbouring agents local activity exploring search space. following alba troya classification linkage approach resembles fine grained topology parallel gas agent communicates neighbours solving problem. details algorithm solve agents problems followed illustrative example. algorithm shows step step procedure proposed method agent involves maximisation problem. beginning number agents population size maximum number generation maxgen convergence tolerance specified. agent devoted search trajectory formed population size line pop matrix populated randomly. agent neighbours set agents line individual population agent composed decision variables agent decision variables neighbours fixed size words equals number neighbouring agents affecting decision agent plus one. agents determine best response decisions neighbours individuals population undergoes reproduction generation parallel searches line pop sorted increasing order based objective value pop best individual pop determined line 12. end generation neighbouring agents neighbours share best individuals form updated population generation pop migrates population neighbours remain fixed generation line 13. makes agent end generation informed decisions neighbouring agents involved problem. example population sharing scheme illustrated fig. agents dealing problem optimising different search trajectories algorithm allows agent search best response neighbours decisions relying locally available information received procedure leads evolution separate populations successive generations. algorithm parallel search algorithm. stopping criterion follow work sinha al. algorithm uses variancebased convergence indicator defined denotes variances agents variables current original population respectively. value restricted convergence expected value converges zero. algorithm terminates algorithm continues maxgen reached. track mean population agents variables generation analyse convergence. constraint domination technique adopted constraint handling 39. procedure ranks feasible individuals based objective values infeasible individuals ranked based extent constraint violation. agent determines best decision response decisions neighbours applying search population line paper differential evolution adopted reproduction provides promising results numerical test problems 41. search algorithm exploited. corresponding individuals randomly chosen population pop new vector created adding weighted difference given mutation rate. accepted new vector following satisfied rand probability crossover rand pseudo random number vector violates bound constraints gets value half way bound violated. test algorithms application explain implementation simple agents problem follows. illustrate algorithm procedure consider agents competing producing product. benefit agent given represents unit production cost incurred agent benefit agent depends production other. interconnection set 105 maxgen 100. seen fig. initialising population generation best individual pop represented pop 22.938 fixed pop generation pop 50.259 fixed pop shown grey black colours respectively. population undergoes reproduction stage second generation pop 16.583 fixed pop pop fixed pop generation. algorithm runs best population fixed iterative basis. simulates procedure agent reacting best action competitor. competition generation designs learning environment continues till agents compromise solution. fig. shows individuals population simplicity presentation. algorithm converges generations 106. section investigate algorithm performance multidisciplinary problems. solve problems related environmental water pollution fluid dynamics. problem involves agents competing water abstraction second approximates pattern velocity profile aerofoil. problem taken deterministic algorithm devised obtaining solution. second problem employs finite difference method formulating agent problems best knowledge solved multiagent perspective literature. problems population size agent maximum number generation maxgen 100 following price al. values equal 0.7 0.5 respectively. convergence assumed tolerance 105. problem firms located river abstracting water produce paper pulp chosen level paper production firms produce pollutant emission coefficient firm table pollution expelled river reaches monitoring stations located river local authority sets maximum pollutant concentration levels following constraints location defined decayandtransportation coefficient firm location table firm engaged net profit maximisation equivalent following optimisation problems firm max subject revenue cost functions firm respectively economic constants 3.0 0.01. cost function coefficients given table 100 study. objective functions represents gross benefit firm participating paper pulp production. complementarity condition essentially state equilibrium solution marginal revenue equal marginal cost firm abstracting positive quantity water marginal revenue exceed marginal cost firms prefer business. function decision firm depends pollution constraints handled domination technique feasible solutions preferred feasible solutions higher gross benefit preferred infeasible solutions violates pollutant constraint favoured. parallel search constructed agents results application shown table solution consistent obtained deterministic approach reported krawczyk 42. convergence rate algorithm provide mean water abstraction agent fig. value algorithm generation. figures illustrate firms competition water abstractions population evolution. initial generations firm gain leads loss oscillating behaviour reach equilibrium generation. convergence fast seen figure generation firms compromise solution algorithm tunes solution afterwards. approach employed bigger problems large number firms compete water abstraction different institutional quality constraints. demonstrate algorithm alter production cost nondifferentiable nonconvex fixedcost function solve problem constraint. introduction function reduces applicability deterministic approaches convexity differentiability assumptions employing proposed method algorithm converges 75.7 71.7 75.7 generations. problem study behaviour algorithm applied fluid dynamic application. solve laminar flow leading edge symmetrical aerofoil naca 0015 zero angle attack chord length 32cm max22.4cm. characteristics aerofoil documented abbott doenhoff exhibits wellbehaved laminar flow leading edge low angles attack. consider flow twodimensional incompressible fluid defining constant density fluid. near low angle attack results zero lift steady flow past aerofoil pattern upper lower surfaces narrow boundary layer region close surface flow retarded friction. resultant simplified navierstokes equations steady twodimensional incompressible flow found 44. practice applying potential flow theory find velocity profiles outer flow field aerofoil 44. subject current study multiagent perspective approximate pattern velocity profiles passing leading edge aerofoil ignoring boundary layer effect. impermeability condition plate surface velocity surface zero gradient velocity equals zero ydirection. conditions rise following complementarity problem velocity profile leading edge naca 0015 aerofoil. obtain pattern velocity profile proposed parallel search approximate different locations surface employing finite difference method mesh step size gradient function approximation leads following set equations location number nodes approximate coordinates node considered agent communicating value neighbouring node problem velocity profile approximation multiagent system following problem agent min subject approaches minimum value zero complementarity condition holds velocity surface zero gradient velocity equals zero ydirection. employ parallel search number nodes approximate agent problems solved different locations leading edge surface shown fig. solve problem consider nodes approximation set account regular mesh node communicating neighbours. convergence analysed looking fig. fig. shows pattern velocity profiles leading edge different locations. distance aerofoil average velocity aerofoil beneath it. expect similar flow profile sides aerofoil. variancebased metric fig. implies pretty fast convergence iterations explaining black circle point location shown fig. communicates position neighbours convergence. opposed population based methods number population grows dimension problem increases ability scale performance challenge deterministic algorithms gradient search approaches employed. test scalability proposed population based algorithm higher dimension problems use velocity profile approximation problem solve nodes agents. account variability run solve problem times results presented averaged runs. report average number generations reach convergence tolerance 105 average time taken convergence. scalability fig. suggests mild exponential trend computational burden number nodes agents increases. 0.5 seconds takes average solve problem average number generations average running time reaches seconds number 104 generations. fig. presents situation illustrated fig. 4a. fig. indicates accuracy approximation increased compared fig. leads higher computational time convergence fig. 5d. report performance proposed approach velocity profile approximation problem search algorithm replaced simple genetic algorithm genetic algorithm elitism gae covariance matrix adaptation evolution strategy cmaes 46. gae use roulette wheel selection mutation rate 0.2 population size 10. generation cmaes new candidate solutions sampled population size according multivariate normal distribution weighted combination best new candidates update distribution parameters. reader referred provided references details algorithms. note simple gae cmaes operators agent use solve problem result communicating neighbouring agents linkage let run algorithm generations. use value performance metric study convergence characteristics. fig. shows convergence behaviour algorithms. seen search operators close solution. cmaes converged runs gae requires runs converge. cmaes smoother convergence behaviour gae quick drop sum average population agents variables generation indication convergence fast. paper developed evolutionary approach solve optimisation problems simultaneously. type representation model engineering economic problems. problem simple solving system equations hard nonlinear combinatorial problem depending properties involved functions. built parallel genetic algorithm separate populations different agents simultaneously evolved dealing optimisation problem. agents partial information model generation synchronise best fitted individual population neighbours. keeps agent aware agents decisions contact system. procedure guides search compromised solution. algorithm illustrated simple twoagent problem performance tested multidisciplinary problems environmental water pollution fluid dynamics results demonstrate applicability proposed technique low high dimension problems. investigated performance approach different population based methods subproblem search algorithm. study suggests different algorithms exploited agent solve problem. study agents consider interest solving local problem interacting other. applicability technique elaborated agents cooperates solve global problem. addition parallel nature algorithm motivate research designing distributed architecture implementation algorithm multiple cores threads synchronisations. framework extended consider different problems water market different users trading water rights 474953 resource constrained environment plasma actuator study velocity profiles induced dielectric barrier discharge plasma actuator necessary represented 505254. acknowledgements anonymous reviewers provided comments suggestions strongly improved content presentation paper. errors omissions authors alone. references',\n",
              " 'fabrication threedimensional structures gained increasing importance bone tissue engineering bte field. mechanical properties permeability important requirement bte scaffolds. mechanical properties scaffolds highly dependent processing parameters. layer thickness delay time spreading powder layer printing orientation major factors determine porosity compression strength printed scaffold. study aggregated artificial neural network aann investigate simultaneous effects layer thickness delay time spreading layer print orientation porous structures compressive strength porosity scaffolds. optimization methods applied obtain optimal parameter settings printing tiny porous structures real bte problem. particle swarm optimization algorithm implemented obtain optimum topology aann. pareto optimization determine optimal setting parameters fabrication scaffolds required compressive strength porosity. results indicate acceptable potential evolutionary strategies controlling optimization 3dp process complicated engineering problem. introduction additive manufacturing layeroverlayer manufacturing technique. cases enables complex components manufactured difficult fabricate conventional methods. practices powderbased threedimensional printing 3dp capable technique bone tissue engineering bte applications 16. seeding cultivating scaffolds bone cells standard method bte. scaffolds highly porous structures aim imitate natural extracellular matrix ecm bone temporary basis. technical point view scaffold engineering sets high demands design materials. addition chemistry interconnected porosity permeability mechanical strength critical parameters define performance scaffold. factors controlled precisely conventional fabrication processes 79. immense potential fabrication scaffolds maximum control porosity ability reproduce customized anatomical design great fidelity medical pictures main advantages powderbased 3dp 1012. fig. shows schematic illustration 3dp process. chosen physical object modeled computeraided design cad system. cad model converted stereolithography stl file format. software program analyzes stl file mathematically slices model cross sections based selected layer thickness. cross sections recreated reaction powder binder. process repeated layer layer object similar design formed. fabrication process printer head jets liquid thin layers powder according object profile created software. subsequently build chamber buildbed containing powder bed lowered enable spreading powder layer. following consecutive application layers unbound powder removed produced 1317. setting 3dp process parameters complex timeconsuming task variables influence printed quality particular applications. cases variables conflict other. recent years reports 3dp fabrication bte scaffolds critical process factors parameters 1822. studies focused improving dimensional accuracy mechanical properties 3dprinted objects shown sensitive process parameters tuned improve desired attributes. characteristics related process parameters improved proper adjustment 419202325. number successful production experiments conducted quality assessment fabricated parts remains main challenges. factors influencing quality studied diverse indicators. significant work focused mechanical properties porosity fabrication tiny pores scaffolds application bte. cost end products process high. technological economic point view selecting process parameters optimization manufactured parts highly essential. context 3dp process optimization improving performance prototype soft computing method promising approach monitor model process based physical understanding experimental data 26. achieving optimal process parameters fabricating parts experimental tests timeconsuming costly approach. numerical models process effective tools finding appropriate process parameters according demanded characteristics. physical modeling point view 3dp process complex. physical phenomena e.g. powder binder reaction removing unbound powder affect product quality. based authors experiments analysis observed relation porosity compression strength porous structures influential parameters nonlinear uncertain. hand formidable task provide authentic exact physicsbased mathematical formulation effectively represent effect layer thickness delay time spreading powder layer printing orientation porosity compression strength porous structures. solving related governing equations analytical numerical methods obtain mathematical model 3dp process difficult impossible. overcome problem best way use soft method obtain datadriven mapping system approximately analyze destined properties porous structures. researchers prefer use semiexperimental models instead numerical models model physical process 3dp process. artificial neural network ann 2730 fuzzy system 3132 hammersteinwiener 3334 time series kalman filter wellknown methods establishing experimental model system based available experimental data. select soft method reliably case study authors considered techniques conducted primitive study neural networks polynomials splines etc. published papers anns suggest modeling methodology promising alternative tool process modeling 3540. method overcome conventional modeling difficulties advantages ease implementation capability constructing complex nonlinear map inputs outputs system. studies conducted ann modeling 3dp process. research aims developing experimental based predictive model printing process aggregated artificial neural network aann method. aann algorithm wellknown variants neural networks models engineering applications 4145. aggregating multiple neural improving generalization neural networks main contribution. researchers shown accurate predictive model obtained comparison single neural network number neurons aggregating neural networks 4647. finding single neural network model highly uncertain complex engineering phenomenon difficult. major drawbacks artificial machines result fitting high computational complexity. combining set independent networks cooperative learnable agents appear promising strategy enhancing robustness generalization artificial machines. promising aspect designing modeling machine find system handle task simultaneously. knowledge authors ensemble artificial machines best suited case single network concentrate modeling specific task neglecting others. predominance aann modeling multioutput phenomena reported studies 46. recent study furtuna al. developed stacked neural network snn evolutionary hyperheuristic method optimum modeling complex chemical process. results imply obvious advantages aanns modeling complex engineering application. selecting best topology main drawback aann. training aann needed select number neural networks number neurons hidden layers neural network. convenient approach exists optimal design systems. researchers proposed different methods grasping optimal topology aanns. zhou al. applied simple genetic algorithm named gasen showed gasen generate aggregated neural network far smaller size stronger generalization ability compared common techniques. study developed nondominated sorting genetic algorithm highlevel heuristic algorithm wellknown propagation method called quasi newton training lowlevel heuristic algorithm optimizing structure aann 47. reported effectiveness method limitations high computational time complexity. study metaheuristic algorithms supervised algorithms finding stage optimum topology aann. metaheuristic algorithms populationbased artificial methods widely handle reallife hard nonlinear engineering problems 4849. algorithms initiate natural evolutionary mechanisms advantages compared conventional methods. particle swarm optimization pso algorithm selected algorithm constructing optimal topology aann. select pso authors considered potential training methods pso abc fa. observed pso high computational speed result computational stagnation. case abc relatively complicated algorithmic structures activate explorationexploitation operators iteration. given fact training aann timeconsuming task pso effectively balance exploration exploitation searching period selected fit algorithm evolving architecture aann. furthermore authors experiments revealed atleast current case study pso afford best results related inertia weight converge acceptable solution logical period time. case abc. parameters e.g. number elite chromosomes mutation probability crossover probability etc. taken account result complex optimization algorithm. observation valid abc need finetuning parameters number employed onlooker bees number limits abounding cite. simulations clearly demonstrated pso faster robust explorationexploitation procedure guarantee convergence near optimum structure aann case methods. observations brought authors conclusion spite simplicity pso logical choice evolving architecture aann. main objective present study develop best aann model analyze nonlinear effect printer machine parameters compressive strength porosity printed porous structures widely challenging aspects printing scaffolds. best knowledge authors aggregated structure proposed considered case study. highly necessary sure developed soft sensor possesses acceptable generalization number data points limited applications possibility fitting fitting makes softsensor unreliable unseen data testing phase. way experiments current study stride indicating aggregated structures best suited applications scaffold modeling easy gather rich database takes long time years come exhaustive database fed simple soft models anfis. rest paper organized follows. section structure printing parameters presented briey. aggregated artificial neural network structure introduced section 2.4. section 2.5 authors provide stepwise explanations particle swarm optimization algorithm. experimental numerical results given section finally paper concluded section previous studies process parameters printing direction axial direction aligned directions printing buildbed greatest impact 2550. adequate pore size bte generally reported range 100800m cell attachment vascularization 51. ability printer fabricate minimum geometrical size restricted powder particle size determines thinnest layer thickness. 3dp processes layer thickness refers height powder bed spread zaxis procedure. typical layer thickness generally twice powder particle size dimension approximately 100m 352. important factors building tiny pore size engineered scaffolds layer thickness 1819. furthermore spraying binder drops causes shear forces applied layer powder bed. result thin printed structures displaced possibly affecting integrity accuracy printed object. mechanical features affected important factor stability predeposit powder layer reaction powder binder 5354. adequate time spreading layer jetting binder start spreading relax desensitize powder binder important factor. refer delay delay time. increasing powder delay time particularly pores channels results densification. added time affect layertolayer bonding consequently affect mechanical dimensional features specimens. finding adequate time fabrication smallscale parts challenging. study scaffold considered cylindrical structure shaped extruding cut small cubicle elements determine pore strut size. scaffold prototypes 12mm height 6mm diameter 0.8mm pore size 0.6mm strut size designed design software solidworks2012 exported stl file. height diameter circumscribed number pores porosity cad model 45.04. geometry scaffolds chosen represents typical feature sizes found bte scaffolds. fig. presents cad model scaffolds. 3dprinting machine zprinter450 zcorporation burlington usa produce prototypes. highperformance composite material zp150 waterbased binder zb63 supplied corporation 5055. printing samples dried 90min machine ambient temperature. printed porous bodies depowdered compressed air remove trapped unbound powder. study authors avoided post hardening infiltration. 3dp process parameters examined study layer thickness delay time spreading new layer build orientation. machine setting parameters default binder saturation core shell runs considered 100. experiment plan based factorial design experiments doe print scaffold prototypes. layer thickness selected possible values 102 114 127m. chosen delay times 100 300 500ms considered orientations. porous scaffold required bte act ecm guide cell proliferation differentiation eventual tissue growth. fluid flow bone scaffold permeability important factor ability build living tissue. successful bte depends scaffolds ability enable nutrient diffusion waste removal regeneration site provide appropriate mechanical environment. words maximum permeability needed far mechanical properties compromised. tradeoff exists requirements 56. note open interconnected pores contribute permeability cell ingrowth closed pores reduce strength. conclusion need minimize closed porosity maximize open porosity way mechanical properties strength modulus compromised. reconstruction complex bone defects osteoporotic fractures patientspecific bte implants proper internal structure mechanical properties needed. fabrication point view maximum compressive strength maximum fidelity cad design maximum permeability needed print scaffold. features evaluate quality printed scaffolds doe test samples. diameter height fabricated samples measured mitutoyo digital caliper smallest measurement 0.01mm. study considered degree anisotropy factor evaluating da. corresponded fully isotropic samples tended samples increasingly anisotropic. values reported ctanalyzer software. scanner experiments highresolution compact desktop unit skyscan vivo xray 1076 belgium. cad design symmetrical printed scaffold lower possess greater sample larger da. uniaxial compressive testing conducted intron 5848 micro tester usa instrument loadcell crossheadloading rate 0.5mmmin1. specimens doe test run tested. porosity reported skyscan microcts analysis software skyscan vivo xray 1076 belgium. resolution scanner set 18m aluminum 0.5mm set filter. region interest considered 66. 700 scan slices taken specimen. section discusses structure proposed model. previously mentioned aann model structure predicting model experimental data training it. standard topology aann shown fig. composed single feedforward neural networks outputs added weighting coefficients. important factor designing aann find fittest weighting coefficients manner aann best performance minimum prediction error lowest complexity minimum size simultaneously. generalized performance neural network expressed following formula error msetrain msetest mean squared errors training testing steps. zero ideal performance. practical applications obtaining exactly zero error impossible authors expect acceptable generalization network error closer zero. mean squared error defined number aann outputs number training testing data. parameters represent actual output aann desired aann output target respectively. individual neural network estimates compressive strength open porosity independently actual outputs aann derived linear superposition independent outputs. independent outputs accumulated mathematically form actual outputs jth output stacked neural network nnno number independent networks jth weight output kth independent neural network respectively. computational complexity objective values designing aann. reducing computational complexity equivalent designing network lowest number neurons hidden layers. following criterion defined metric complexity complexity number neurons cat independent neural network. obtain model lowest prediction error computational complexity authors define following total objective function scaling factor represents degree importance objective function. lowest value means complexity important prediction error. paper limited actual data available complexity important efficiency. noteworthy complexity increases possibilities fitting. constant value 0.5 considered. correlation actual desired output data important factor designing aann. training procedure way highest correlation value occurs actual desired data. purpose minimum acceptable value correlation imposed training phase constraint. mathematical expression correlation follows 0.9 correlation average actual values obtained processing element neural network output average desired values processing element neural network output. entering training phase experimental data normalized according following equation 123 actual input normalized input lower bound upper bound ith input respectively. worth noting upper bound lower bound values maximum minimum values actual input data. equation acceptable range inputs 11. preprocessing procedure conducted outputs. pso algorithm populationbased soft computing technique attracted attention researchers solving applied engineering optimization problems. algorithm based behavior swarm ants flock birds school fish mimics social behavior finding food actions encountering danger. social behavior developing modern optimization algorithms. kennedy eberhart originally proposed pso algorithm 1995. algorithm solution optimization algorithm considered particle swarm particles located randomly feasible searching domain initially selected. particles i.e. candidate solutions updated evolutionary mechanism obtain better solution. proposed evolutionary mechanisms similar happens nature. nature particle tends act similar best experience life moves best successful experience neighbors. behaviors called exploration exploitation respectively computationally implemented follows. pso algorithm optimization problem min particle presented position vector generation time jth variable ith particles swarm size searching space dimension. solution optimization problem swarm set swarm formulate evolutionary mechanism velocity vector assigned particle follows velocity ith particle. velocity specifies updating direction updating rates particle position. particles assumed iteratively search space. best experience particle time stored variable called best position expressed n1i set memory set shows best positions swarm explored particle. evidently best position lowest fitness function value set global best solution minimization problem. global minimal position represented computed arg min classic variant pso updating rule mathematically expressed follows weighting coefficient cognitive social acceleration coefficients respectively random numbers uniformly distributed 01. according relation weighting coefficient gives inertia behavior motion particle. higher values result exploration behavior lower value increases exploitation performance. parameter controlled generation time. updating velocity particle particle adjusts current position following relation paper pso constant inertia weight aann structural parameters considered particle parameters swarm. fact particle create aann network return network error eq. pso min function. 0.5 nnno number single neural networks number weighting coefficients number hidden layers single neural network. topology aann coefficients hidden layers depicted fig. details given section. results discussion section predictive model 3dp process extracted. model predicts mechanical strength open porosity fabricated process. mechanical strength open porosity fundamental properties tissue engineering applications. influencing parameters build orientation delay time layer thickness. sufficient green strength necessary depowdering step compressive strength decreases need postprocessing. input parameters aann model include orientation thickness delay time output parameters mechanical strength open porosity. required identification data prepared conducting experimental tests different values permutations input parameters. orientation input axis directions. layer thickness levels 102 114 127m chosen delay times 100 300 500ms spreading layer. total number experimental tests 48. samples printing drying 1.5h depowdered characterized compressive strength porosity. table shows experimental data printed porous structures training aann model. fig. shows effect parameter compressive strength. compressive strength printed porous specimens printed direction significantly higher specimens printed directions. insufficient compressive strengths zdirection fabrication led fracture samples depowdering. furthermore samples printed 300ms delay time spreading layer compressive strength compared samples. shown fig. effect input process outputs complicated nonlinear meaningful trend exists them. xprinted samples 89m 114m delay time 300ms spreading layer strength printed samples. direction print direction applied compression load significantly affected strength specimens. loads applied parallel layers printed structure fabricated axes applied perpendicular layers fabricated zprinted samples. addition cross section samples mathematically sliced printed layeroverlayer circular specimens cylindrical. samples printed direction fabricated layers completed samples printed directions needed overlay fewest layers cross section. layer displacement occurred zprinted samples resulted distortion fractures struts. fig. shows effect setting parameter porosity printed specimens. specimens printed direction 114m layer thickness 100ms delay spreading layer specimens printed direction 102m layer thickness 500ms delay open porosity. total specimens printed directions porous prototypes. porous scaffolds desired compressive strength compromised. fig. shows effect input process outputs. effect complicated nonlinear meaningful trend exists them. open porosity refers macro micro pores. cad model porosity 45.04 increase open porosity related micro pores powder particles. proposed topology aann model includes nnno independent variables. relation nnno number single neural networks nnno number weighting coefficients. aside unknown parameters single neural network unknown. number neurons hidden layer single neural network weighting biasing coefficients undetermined. variables obtained manner prediction error small possible. training process conducted levels. lower level single neural network trained according wellknown propagation algorithm called quasi newton learning technique. study 300 epochs considered stopping criterion quasi newton training method. higher level called supervising level pso algorithm supervising optimization algorithm find optimal topology. supervising level number single networks limited input vector independent variables respectively. variable indicates weighting coefficients number hidden layer single network. find best number single neural networks structure aann process implemented number aanns different number single neural networks. method quasi newton learning technique train single networks 300 epochs considered criterion stop procedure. experience shows combination pso quasinewton method useful tool find optimal topology aann. independent variables solution vector listed table optimum solution aann shown table indicates topology single neural networks. consequently weighting coefficients best aann model 3dp process. number neurons hidden layers optimum aann structure found pso applied structure find optimum weighting coefficients process leads accurate solution. solution shown table sure selected parameters stable simulation selected parameters carried independent runs observed variation final solutions independent runs trivial anticipates robustness searching mechanism. fig. shows training testing accuracy obtained aann predicting mechanical compression strength. figures aside compression strength normalized value regression analysis mean square predicting error normal distribution predicting error presented training testing. clearly prediction error acceptable range resulting aann network reliable predicting compression strength. similar compression strength results shown fig. open porosity parameter. unlike compressive strength open porosity regions testing match training data higher prediction errors seen. errors indicate open porosity sensitive parameter mechanical strength uncertainties influence it. example internal channels blocked filled unbound powder imperfect depowdering step porosity decrease. uncontrolled effects produce distributed data. obtaining highly correlated data possible. fig. shows variations open porosity respect variations delay time layer thickness different depositing directions. behavior open porosity complicated nonlinear. related surfaces orientations effective open porosity delay time thickness significant. mechanical compression strength results obtained variation mechanical strength result process parameter variations. fig. shows mechanical strength behavior time delay thickness vary orientation kept constant. contrast open porosity nonlinearity mechanical strength low deposition orientation effective obtained mechanical strength. step aann model optimal designing 3dp process parameters fabricate desired scaffold. mechanical strength open porosity fitness functions multiobjective optimization algorithm used. multiobjective optimization context pareto comprehensive solution. knowing pareto designer select desirable solution based imposed constraints. research pareto 3dp process scaffold fabrication shown fig. according pareto open porosity conflicts mechanical strength way increasing open porosity result decrease mechanical strength vice versa. figure summarized table shows orientation layer thickness effective delay time effective parameter. obtain scaffold highest mechanical strength delay time selected 200ms 135ms achieve highest porosity. conclusion study aann investigate simultaneous effects layer thickness delay time spreading layer print orientation compressive strength porosity porous structure prototypes. optimization methods applied obtain optimal parameter settings printing tiny porous structures real bte problem. pso algorithm implemented obtain optimum topology aann. pareto optimization determine optimal setting parameters fabrication porous structure high compressive strength porosity. relationship layer thickness delay time spreading layer print orientation powderbased printed scaffolds compressive strength index mechanical performance porosity discussed. annbased model powerful tool predict compressive strength porosity 3dprinted scaffolds wide range layer thicknesses delay times spreading layer print orientations limited set experiments designed factorial design experiments. results predict best mechanical strength porosity based setting parameters. contribution current study trained model precisely analyze properties scaffolding procedure. presented results discussion informative information practitioners want design porous structure need know impact influential design parameters e.g. layer thickness delay time spreading powder layer printing orientation porosity compression strength porous structures. acknowledgment study supported high impact research grant ministry higher education malaysia um.chirmoheeng10 d00001016001. references',\n",
              " 'global optimization mining complexes aims generate production schedule mines processing streams maximizes economic value enterprise whole. aside large scale optimization models major challenges associated optimizing mining complexes related blending nonlinear geometallurgical interactions processing streams materials transformed bulk material refined products. work proposes new twostage stochastic global optimization model production scheduling open pit mining complexes uncertainty. combinations metaheuristics including simulated annealing particle swarm optimization differential evolution tested assess performance solver. experimental results coppergold mining complex demonstrate optimizer capable generating designs reduce risk meeting production targets 6.6 higher expected net present value deterministicequivalent design 22.6 higher net present value industrystandard deterministic planning software. introduction global optimization mining complexes addresses issue integrated mining processing operations multiple pits underground mines multiple metals minerals stockpiles blending options alternative processing streams yield distinct products 12. primary objective mining enterprise maximize net present value npv cash flows requires optimizing longterm extraction sequences use materials mined. extraction sequences define inventories raw materials produced mines. downstream optimization defines use mining complexs processing streams maximize utility available materials addresses destination policies send material mines processing stream decisions send stockpiled processed material. historically components optimized independently leading suboptimal solutions mining complex existing attempts global optimization ignore compounded effect uncertainty i.e. geological economic value operational feasibility supply chain 67. complexity supply chain increases respect number mines processing stream options methods distribution increasingly necessary integrate elements simultaneously considering uncertainty arises mining complexs components. recent work focused integrating geological supply uncertainty open pit production scheduling optimization models. ramazan dimitrakopoulos propose twostage stochastic integer programming sip formulation seeks maximize npv production schedule minimizing risk meeting production targets. risk discounting parameter optimizer aims strike balance extracting highvalue lowrisk material beginning mines life deferring riskier material later periods information available. basic sip model tested improved 1014 results consistently demonstrate npv production schedule considers geological uncertainty substantially higher conventional schedule direct result managing impact risk ensure production targets attained life business. previous formulations assume apriori decision ore valuable waste material commonly referred cutoff grade policy 1516. past research attempted integrate dynamic destination decisions longterm deterministic production scheduling exploiting structure linear optimization model 1720. boland al. propose multistage stochastic optimization model decides destinations scenario overlyoptimistic assumes perfect knowledge material extracted allocated available processing streams. research efforts investigated use scenarioindependent cutoff grade destination policies scenarioindependent block destinations 2324. methods severe limitation. cutoff grade policies primarily useful mining complexes treat single metal material quality constraints processing streams. scenarioindependent block destinations leads misrepresenting inherent operational flexibility change block destinations based shortterm grade control information. results material misclassification waste sent processing. order provide useful destination policy mining complexes general necessary develop new approaches define destination policies uncertainty. underlying challenges globally optimizing mining complexes inherent nonlinearity related blending stockpiling materials transformations occur refining bulk input material set output products. attempts optimize mining complexes 122633 models ignore geological uncertainty limited degree flexibility modeling nonlinear transformations processing streams. existing optimization approaches avoid nonlinear nonadditive geometallurgical interactions processing streams preprocessing economic values recoveries throughput rates attributes interest individual block. assumes block processed independently unrealistic assumption mining complexes require blending homogenization prior processing. provide alternative assessment economic viability mining operation necessary explore new models shift focus economic value blocks mined economic value products sold customers ultimately permits integrating nonlinear processing transformations directly optimization model. metaheuristics provide useful platform global optimization ability handle largescale nonlinear optimization models. increased effort adapt existing metaheuristics open pit design production scheduling problem 14243441. methods consistently demonstrated ability optimize largescale production schedules reasonable time works limited defining destination policy priori optimize singletier processing stream configurations i.e. single final processor stockpiles multiple refinery options ports customers. work focuses global optimization open pit mining complexes supply uncertainty addresses limitations previous work. new type destination policy proposed useful decisionmaking uncertainty mining complexes blending constraints secondary elements. new global optimization model developed model multitier processing stream configurations nonlinear transformations. following section overview modeling approach given. subsequently novel twostage sip formulation proposed firststage decisions optimize extraction sequence destination policies secondstage recourse decisions optimize processing streams supply chain. following metaheuristic solvers discussed use combination simulated annealing particle swarm optimization differential evolution. solvers tested compared coppergold mining complex. finally conclusions future work presented. section discusses generalized modeling framework mining complexes uncertainty. definitions materials related attributes context mining complexes uncertainty provided. following decision variables govern extraction sequence destination policies processing streams discussed. mining complex materials products mined generated blending processing. materials considered unique mineralogy metallurgical attributes result sent set certain locations mining complex treatment. order define flow materials sources mines final products refined metals useful describe mining complex directed graph. let graph represent flow materials mining complex. set nodes comprised disjoint subsets clusters materials mines similar attributes e.g. metal content. section2.3 detailed description. destinations able stockpile material time. input material treated transformed nodes. destinations process transform send output material subsequent nodes available. attributes quantify information interest optimization model metal quantities costs. uncertainty attributes quantified set joint scenarios scenario defines realization sampling sources uncertainty. attributes categorized classes primary attributes fundamental variables interest entire model e.g. metal total tonnages sent node node quantity attribute denoted originate mines flow mining complex final products. attribute recovered treatment denoted pits hereditary attributes information relevant optimization model necessarily passed node e.g. feed material chemistry processing costs throughput rates energy consumption revenues sales. attributes expressed non linear functions primary attributes quantity hereditary attribute denoted mines suppliers bulk materials mining complex represented set discretized volumes material called blocks quantify geological uncertainty materials attributes assumed block simulated material classification simulated attributes pbs fig. shows crosssection stochastic simulations realworld coppergold deposit case study section4. noted large differences material classifications copper grades block simulations. extraction sequence determined decision variables define block extracted period order safely extract block necessary uncover extracting set overlying blocks entirety. overlying blocks identified block preprocessing step creating inverted cone center verifying blocks lie inside cone. complex cases multiple geotechnical zones deposit requiring variable slope angles north east south west walls. fig. gives example overlying blocks defined variable slope angles. detailed description preprocessing algorithm reader referred khalokakaie al. 42. work material mines decomposed subgroups based attributes similar quantities e.g. valuable deleterious metal content. destination policy outlines subgroup material sent scenarios. similar concept introduced menabde al. separate univariate distribution metal content grades bins i.e. categories based ranges metal content create timevaried cutoff grade policy based bins fig. looking individual blocks 212324 optimizer requires substantially fewer decision variables focuses distribution grades. general case proposed subgroupings materials called clusters created multivariate distributions permit higher degree flexibility defining policies fig. 3b. cases destination single block change simulations depending simulated attributes. general method addresses limitations cutoff grade destination policies able consider impacts deleterious elements blending stockpiling performance value chain. order define destination policies necessary classify simulated blocks clusters kmeans clustering algorithm 4344 useful method grouping information similar attributes. algorithm creates predetermined number cluster centroids material based simulated attributes. given scenario blocks cluster membership determined closest centroid measured euclidean distance blocks simulated attributes centroids multivariate attributes. cluster membership subject change block given material classification attributes vary simulations. let bcs represent preprocessed parameter defines block member cluster scenario destination policies determined variable cjt represents decision cluster sent destination period noted set candidate destinations determined type material cluster belongs to. destination policy variables described section2.3 define send material mined. depending configuration processing streams given mining complex necessary model transfer materials locations. processing stream decision variables ijts define proportion output material sent destination period scenario noted unlike extraction sequence destination policies decisions designed adaptive uncertainty material received initial destination uncertainty revealed assumed mining complex adapt appropriately. given primary attributes interest assumed linear additive recovered quantity primary attribute sent destination calculated similar existing deterministic models longterm production planning primary objective maximize npv 13426. introduce risk management critical objective proposed mathematical formulation based existing twostage stochastic integer programming models developed planning 8101113. proposed model discussed differs substantially existing sip formulations generalized optimize aspects mining complex holistically. longterm extraction sequence destination policies defined firststage decision variables designed robust fluctuations arise uncertainty geological attributes. recourse secondstage variables adapt firststage decisions include processing stream variables penalties excessive risk inability meet specified targets. determine discounted revenues expenses attribute mining complex directly included objective function associated discounted value hit hi11 discount rate. deviation variables penalized related penalty costs respectively. similar discount rate calculate npv penalty costs defined monotonically decreasing respect time i.e. phenomenon referred risk discounting attempts defer riskier material later periods life. risk discount rate parameter describe modelers desire balance ability meet production targets short longterms. penalty costs relate willingness pay unit deviation capacity constraint determined experimentally running optimization model multiple times obtain desirable risk profile constraints interest 12. general global optimization formulation open pit mining complexes defined follows objective max discounted revenues costs riskdiscounted penalties deviations subject capacity constraints calculate value hereditary attributes evaluate surplus shortage upper hit lowerbound hit locations mining complex. examples typical constraints include limited production capacity stockpile capacity processing hours grade blending constraints. reserve block access constraints ensure block extracted overlying blocks extracted. destination policy constraints calculate quantities primary attributes cluster blocks ensure cluster sent destination mining complex. extraction constraints determine quantities primary attributes extracted e.g. annual tonnage. processing stream flow constraints calculate quantity primary attributes retained received location ensure mass balancing materials sent stockpiles processors subsequent destinations mining complex. endofyear stockpile quantities calculate quantities materials remain stockpile end production period. binary constraints extraction sequence enure blocks partially mined given period. binary constraints destination policies variables cjt ensure cluster sent single destination period variable definitions generalized global optimization formulation challenging solve conventional mathematical programming methods particularly models include nonlinear functions. metaheuristics algorithms useful cases require linear formulations special structure optimization problem. metaheuristics guarantee mathematically optimal solution provided useful solutions miningrelated problems 14243436414546. models solution methods extended global optimization mining complexes. simulated annealing 4749 selected base algorithm comparison purposes previous success method extraction sequencing 24343738. simulated annealing discrete optimization problems algorithm adapted accommodate continuous processing stream decision variables ijts 5051. primary concerns applying single metaheuristic possibility getting trapped local optima particularly context mining complex distinct sets decision variables interrelated. ability optimize discrete continuous variables important feature selecting metaheuristics model hand. particle swarm optimization pso 5253 differential evolution 5455 known ability optimize mixed integer nonlinear optimization models need complex encoding decoding schemes. pso extraction sequencing encoding extraction sequence twodimensional surface. decision variables relate depth mined coordinate pair column given period 3641. encoding scheme particularly suitable populationbased metaheuristics initial tests proposed global optimization model purely pso indicate method sensitive initial sequences destination policies generated population. additionally methods require substantial computational effort order normalize extraction sequence enforce slope constraints eq. methods remain appealing particularly optimizing destination policies processing streams i.e. downstream optimization given simultaneously modify sets interrelated decision variables. assess performance basic simulated annealing algorithm additional variants tested simulated annealing downstream particle swarm optimization sadpso simulated annealing downstream differential evolution sadde. algorithm appendix provides overview global optimization metaheuristic developed. computational results coppergold mining complex discussed section4.2. solution vector store decision variables. extraction sequence vector stores encoded version variables discretevalued element represents extraction period block value extraction period indicates block mined. destination policy vector stores encoded version cjt variables element represents encoded destination cluster period range decoding scheme convert value appropriate destination cluster. finally processing stream vector stores ijts variables element maps directly ijts variable lie range initialize algorithm initial sequence initialized blocks unmined destination policies processing stream variables randomly generated bounds. noted eqs. inherently modeled soft constraints original mathematical formulation cases deviations capacity constraints penalized objective function. deterministic models solved metaheuristics 363941 treat penalties tool eliminate constraint violations penalties tool manage distribution risk time stochastic approach 81014. slope constraints eq. guaranteed perturbation mechanism algorithm reserve constraints eq. guaranteed encoding scheme similarly singledestination constraints eq. inherent encoding scheme finally processing stream proportion constraints eqs. guaranteed normalizing values optimizer makes changes. follows constraints automatically feasible simply calculations derived variables. basic algorithm uses modified algorithm algorithm improve global best solution vector classes perturbations solution changes considered annealing extraction sequence perturbations block randomly selected mining period changed possibly mined all. blocks violate slope constraints result change considered 37. destination policy perturbations cluster destination decision variable randomly selected sent different destination possible. processing stream perturbations processing stream variable randomly selected value modified random normal number ijts 0.1. note variance normal distribution sufficiently small permit local global exploration. processing stream variables normalized respect eqs. 14. perturbation mechanism outlined algorithm parameters prob seq prob dest determine probability selecting sequencing destination policy perturbation processing stream perturbation. probability accepting perturbation solution vector maximization problem based following distribution exp objective function values perturbation respectively annealing temperature. algorithm progresses temperature gradually reduced minor changes accepted. controlled initial temperature start algorithm cooling schedule defined reduction factor number iterations reduction factor applied iter fig. shows comparison cumulative distributions changes objective function values nonimproving perturbations neighborhoods. single temperature classic algorithm large temperature optimizer limit number suboptimal changes destination policy likely accept extraction sequence processing stream changes. temperature decreases likely nonimproving processing stream changes accepted. behavior desirable given neighborhoods strongly related. proposed modified algorithm cumulative probability distributions cdf seq cdf dest cdf proc shown fig. constructed proposing random perturbations solution. single temperature neighborhoods eq. optimizer uses single parameter represents probability accepting nonimproving perturbation. fixed correct temperature variable retrieved appropriate cumulative probability distribution lookup function cdf seq cdf dest cdf proc cooling schedule iter applied algorithm progresses information garnered new proposed nonimproving perturbations feedback update cumulative distributions better reflects current search space search space algorithm commenced. global optimization algorithm algorithm complete acceptance probability reset initial value algorithm restarted global best solution diversification strategy repeated improvement found. order assess performance basic algorithm variations pso proposed improve solution downstream variables prior diversifying annealing. outline downstream optimization provided algorithm pso populationbased metaheuristics employ unique approaches modify existing solutions. approaches require defining size population parameter. pso member population particle comprised equallysized vectors solution vector best solution vector best velocity vector initially vectors randomly generated. maintain highquality solution swarm reduce chance premature convergence particles best vector best initialized global best solution previously found annealing algorithm provides outline particles vectors updated. key parameters need calibrated pso represent weights particles inertia best solution localbest solution respectively. noted algorithm localbest ring topology nearest local particles particle used. localbest solution lbest best represents best solution particles adjacent indices i.e. local local determined experimentally delay swarm convergence. algorithm operates similar pso algorithm. population represented set agents. algorithm provides outline agents solution vector modified. order update agent agents randomly selected population. agents best solution vector best modified randomly selecting variables crossed agent weighted difference agents types variations modify solution 5758 determined experimentally simple modification structure provides adequate results. algorithm requires calibrating parameters crossover ratio defines probability modifying decision variable parameter defines weight applied difference solution vectors unlike pso modifies proportion decision variables agent iteration found testing helps prevent population converging prematurely. proposed integrated planning optimization framework demonstrated realworld coppergold mining complex. given case study single supplies materials mining complex produces gold copper. fig. summarizes definition mines materials processing options. contains main material groups sulfides transition oxides. order respect chemistry requirements sulfide heap leach processor sulfide transition material groups separated different material types based 0.2 copper. oxide materials classified ore waste based chemistry. deposits uncertainty represented set equally probable geological simulations variable copper gold tonnages material types. exception oxide waste dump destinations processors variable graderecovery curves based average grade incoming material process given period fig. nonlinear graderecovery relationships interesting implications considering transition materials given block cluster hypothetically similar economic values processing options selected destination profits increase recovery. result assume destinations specified apriori greedy manner recovery aggregated material sent given processor determines potential value. reason model considers economic value block economic values evaluated recovered copper gold processors mill leach pads. objective optimizer maximize npv mining complex considers sale copper gold destinations processing mining costs. costrelated parameters table expressed relative mining cost confidentiality purposes. table summarizes constraints penalty costs deterministic stochastic optimization models. risk discount rate penalize deviations production capacities ensures riskier material deferred later periods geological information available. model contains 34057 blocks scheduled years slope angle 42. deterministic orebody model created averaging grades given simulations reclassifying material types manner simulations classified. model referred etype orebody model 59. basecase design obtained etype orebody model industrystandard commercial planning software package. results optimizing deterministic model proposed optimizer referred deterministicequivalent design. deterministicequivalent model material clustered groups exception oxide waste material considers one. result 1672 encoded destination policy variables 34057 encoded block extraction decisions stockpile processing stream variables table summarizes parameters simulated annealing particle swarm optimization differential evolution algorithms. noted iteration gopt algorithm different set parameters compensate large number new blocks introduced extraction sequence. table shows comparison sadpso sadde algorithms trials performed method. trials run amazons ec2 c4.8xlarge virtual machines virtual cpus intel xeon e52666 ram. trial initialized use initial extraction sequence i.e. downstream solution vectors randomly generated. apparent sadpso sadde algorithms consistently outperform algorithm uses simulated annealing average increase net present value npv 1.91 2.57 respectively. noted final solutions contain capacity constraint violations npv analogous objective function value. slight improvements come drastically longer computing time. average takes 2.4 2.9 times longer run sadpso sadde algorithms respectively basic algorithm. basic algorithm converges average 14.7 diversifications. combined metaheuristic optimize downstream variables number diversification iterations increases. despite sadde algorithm having smaller initial population sadpso agents versus particles average run time higher fact diversifications performed algorithm requires iterations catch improve global best solution optimizing de. fig. left shows comparison sulfide mill sulfide heap leach cumulative discounted cash flows commercial deterministicequivalent extraction sequences. noted deterministicequivalent solution generates 13.8 higher npv solution generated commercial planning tool life extended year. drastic increase largely result global optimization approach production schedule destination policy stockpiles optimized simultaneously commercial approach relies iterative approximation ultimate pit limit phase design definition prior scheduling heuristics order provide production schedule. risk analysis performed taking schedule destination policies generated deterministic optimizer testing geological simulations perform. noted extraction sequence destination policies remain fixed stockpile variables optimized scenario algorithm. fig. left provides risk profiles defined exceedance probabilities p90 p50 p10 respectively simulations responses deterministicequivalent design. simulations indicate deterministic design performs noticeably worse etype orebody model indicates attributed differences univariate grade distributions estimated model simulations. spikes sulfide mill tonnage capacity order 510 related fact simulations contain excessive quantities highgrade material production schedule tonnage spikes unrealistic inflate npv risk analysis. risk analysis indicates deterministic design consistently unable meet production targets sulfide heap leach. interestingly npv risk analysis unaffected inability fill sulfide heap leach capacity. result sending excessive higher valued materials sulfide mill leads increased recovery profits sulfide mill despite underutilization sulfide heap leach. stochastic optimizer aims optimize extraction sequence destination policies use stockpile considering geological uncertainty. table outlines penalty costs constraint bounds guide risk profiles. total trials run stochastic example sadde optimizer. average computing time stochastic optimizer 75.6h standard deviation 8.6h. comparing objective function value best stochastic design simulations best deterministic design calculated penalty costs constraint bounds best stochastic design 16.6 higher value average trials. direct result able manage impact risk sulfide mill sulfide heap leach tonnages time. elucidate results fig. right shows risk profiles stochastic solution sulfide mill tonnages sulfide heap leach tonnages cumulative discounted cash flows. apparent stochastic design capable meeting target capacity 3mtpa sulfide mill average substantially risk deterministic design fig. left. stochastic design meets target sulfide heap leach tonnage periods quantities begin decline. implies riskier sulfide heap leach material deferred end life result incorporating risk discounting stochastic optimization model. finally noted npv stochastic design 6.6 higher deterministicequivalent design comparing p50 values 22.6 higher commercial deterministic planning software. conclusions work presents framework global asset optimization mining complexes uncertainty solutions provide robust longterm openpit extraction sequences destination policies. proposed framework permits high degree flexibility detail modeling mining complex including opportunity integrate nonlinear relationships generally ignored existing models challenges associated nonlinear optimization. result proposed method overcomes major limitation existing global optimization models modeling economic value products sold value blocks mined. new form destination policies developed overcomes severe limitations methods quality constraints secondary elements processing streams. solvers based combination simulated annealing particle swarm optimization differential evolution developed compared. method tested coppergold mining complex. optimizing deterministic example global optimizer achieves design 13.8 higher net present value industrystandard planning tool. optimizing simulated annealing differential evolution demonstrated improved results comes expense substantially longer computing time. stochastic optimization results indicate stochastic design able satisfy target tonnage capacities ensuring able treat profitable material life mine. additionally stochastic solution indicates 6.6 higher net present value compared deterministicequivalent design 22.6 higher value industrystandard software. given proposed method seeks generate single set destination policies future research investigate use multistage stochastic optimization order permit adaptive policies supply geological demand metal price uncertainty likely lead higher economic value. acknowledgements work paper funded nserc crd 411270 nserc discovery grant 239019 industry members cosmo stochastic planning laboratory anglogold ashanti barrick gold bhp billiton beers newmont mining vale. algorithm algorithm global optimization mining complexes. algorithm simulated annealing open pit mining complexes. algorithm perturb current solution retrieve appropriate annealing temperature respect selected perturbation neighborhood. algorithm downstream optimization particle swarm optimization differential evolution. algorithm particle swarm optimization update particle algorithm differential evolution derand1bin agent references',\n",
              " 'support vector machines wide use prediction problems life sciences. shown offer generalisation ability inputoutput mapping. performance predictive models negatively influenced complex highdimensional nonlinear nature postgenome data. soft computing methods model nonlinear systems. fuzzy systems widely methods soft computing model uncertainties. formed interpretable rules aiding gain insight applied model. study concerned provide interpretable efficient biological model development hybrid method integrates fuzzy system support vector regression. order demonstrate robustness new hybrid method applied prediction peptide binding affinity challenging problems postgenomic era diversity peptide families complexity highdimensionality characteristic features peptides. having different case studies hybrid predictive model yielded highest predictive power cases achieved improvement compared results presented literature. availability matlab scripts available httpsgithub.comsekerbigdatalabtsksvr. introduction peptide binding plays vital roles molecular biology cell. process peptide binding activate cytotoxic tcells immune system challenging complex aspect peptide binding prediction proteinpeptide binding affinity. bindings crucial induce cellular immune responses hand diversity peptide families large number peptides available discovered e.g. potentially 512 billion peptides mhc molecule biological experiments measurement binding affinity proteins peptides costly timeconsuming. regard computational methods particular interest bioinformatics finding feasible approaches problem 45. predictive models identification peptide binding affinity find binding exists peptide mhc molecule qualitative models improved focused modelling classify binders strong weak binders 79. recent research efforts focused quantifying binding predictions. additive model earliest quantitative approaches proposed model mhcpeptide finding precise binding affinities 10. studies focused nonlinear approaches achieved better performance compared linear models additive method. nonlinear modelling approach taken number later methods regularisation methods partial squares random forests reveal realvalue binding affinity. complexity nonlinearity exist data sets led necessity robust sophisticated methods. fuzzy systems able model uncertain imprecise knowledge complex nonlinear data sets form structure representing human reasoning. fuzzy systems takagisugenokang tsk commonly modelling complex systems 1415. tsk fuzzy systems tskfs combined methods particularly learning methods enhanced learning adaptation capabilities 16. tsk models rule antecedent form membership functions rule consequent linear function inputs. methods proposed model tskfs general approach premise parameters constant values consequent parameters computed. computation square estimation lse statistical modelling assumes linear relationship exists input output variables. lse based minimising empirical risk constitutes essential tsk fuzzy systems 1718. drawback squares learning algorithm training error minimised model badly suffer overfitting 19. methods explored addressing problems square estimation e.g. neurofuzzy systems geneticfuzzy systems 20. support vector regression svr 2122 efficient robust method provides high generalisability performance. applications svr demonstrated considerably better modelling nonlinear systems minimising structural risk squares approach. considered concept incorporated tskfs order better train consequent 23. methods reported literature utilisation support vector based methods consequent fuzzy system 2426. paper supportvector based takagisugenokang fuzzy system tsksvr proposed applied quantitative prediction binding affinities major histocompatibility complex proteins mhcs peptides important problem biology medicine applications drug design. paper extends initial work improves initial results yielding improvement prediction accuracy presented recently published papers. rest paper organised follows. section2 introduces peptide binding affinity problem. section3 background methodology explained. section4 presents svrbased tsk type1 fuzzy prediction model. section5 presents results discussion. finally conclusions drawn section6. section presents problem statement data sets used. peptide presented mhc class molecules short number amino acid sequence generally contain amino acids 28. peptides bind protein molecules order induce cellular immune responses. affinity indicates tendency strength binding. larger number peptides potentially 512 billion binding peptides mhc molecule need prediction methods help determine binding affinities peptides. addition order avoid time consuming task computational predictive model developed. difficulty peptide prediction problems building prediction model number features large study 5000 number peptides training data set relatively small study 150. highdimensional peptide data sets provided comparative evaluation prediction algorithms coepra modelling competition study order improve predictivity affinity peptides particular test predictive capability proposed tsksvr model given data sets. shown table task contains calibration training prediction test data sets physicochemical descriptors provided small peptide calibration prediction data sets. addition different amino acid data sets literature consists physicochemical biochemical properties amino acids e.g. aaindex database cisaps consistent coepra amino acid peptide described 643 descriptors. noted descriptors picked aaindex database. task consists octapeptides total 5144 64385144 descriptors tasks nonapeptides total 5787 64395787 descriptors table statistics range mean standard deviation binding affinities peptides task given table proposed approach consists number components implemented prediction peptide binding affinity. section provide background information related components. takagisugenokang tsk fuzzy system rules defined conditional statements presented linear function consequent part. fuzzy rulebase input variables rules written fuzzy set input variable rule generally represented membership function linear function consequent expressed coefficients input variables tsk model rule generates crisp output final output obtained aggregating rule outputs. process called defuzzification weighted average defuzzification value defined firing strength normalised firing strength fuzzy rule respectively determined tnorm operator defined membership degree input variable fuzzy sets e.g. described form membership functions. study gaussian membership function expressed centre standard deviation respectively. support vector machine svm statistical learning architecture based structural risk minimisation 32. svm learning algorithm finds optimal separating hyperplane training classifier given training data. optimal separating hyperplane maximises margin classes. svms generalised perform regression linear model. traditional square error loss function insensitive loss function svr 33. chosen error function tolerates errors advantage error function tolerance noise. svr searches linear function represent coefficients weight vector linear expression. linear function constrained following mathematical expressions min subject types slack variables measure deviations training samples region 22. values variables computed training svr parameter prespecified value works regularization factor minimising value deviations greater tolerated. certain training instances chosen support vectors. weighted sum support vectors define regression adequately model data. section presents implementation svrbased tsk type1 fuzzy prediction model. flowchart proposed model shown fig. amino acids peptides form data set turned numerical descriptors amino acid indices. analysis started normalising data set order feature fall range values. descriptors normalised scale interval expressed 10. min max min feature selection process reduce dimensionality choosing subset relevant features leading better performance system model. regard feature selection algorithms widely bioinformatics aiming finding number features improve accuracy performance models 34. feature selection methods available. study problem feature selection addressed utilising multicluster feature selection mcfs proposed model superiority recently shown different application domains 3638. mcfs unsupervised feature selection method uses information contained eigenvectors solving generalised eigenproblem preserve multicluster structure data. study number eigenvectors parameter mcfs set number features selected. fuzzy cmeans fcm method partitions data set number clusters way data object assigned degree membership cluster 39. fcm model aims minimise optimisation function. clustering process iteratively calculates cluster centres degrees memberships data point optimisation function satisfied number iterations reaches preset value. construction rulebase membership functions automate rulebased fuzzy system clustering based methods commonly particular type1 fuzzy systems 4041. fuzzy sets involved rules fully characterised membership functions. explained section3.1 gaussian membership function utilised develop fuzzy rule base. centroids clusters corresponding standard deviations obtained fcm determine values parameters gaussian membership functions. study degree fuzzification chosen fcm number clusters determine number rules. square estimation common method compute values consequent parameters tskfs 18. given support vector regression concept linear kernel potentially utilised compute values consequent parameters tskfs. variables defined normalized firing strength form inputs svr derive parameters correspond consequent parameters tskfs. finally svrbased tskfs formulated combining 12. represents formulation svrbased tskfs. sake simplicity order implement support vector regression libsvm library 42. important parameters likely affect performance models. optimise svr linear kernel number rules i.e. clusters tskfs. fact generally accepted methods exist determine parameters optimally gridsearch method decided employed parameter selection method order find optimal parameter set. gridsearch method simple reliable allows implement parallel computations. parameter range searched step size 0.05 finding optimal svr kernel linear parameters. features search range decided 250. hoped ranges broadly cover possibilities contain optimal measure. parameters different combinations features assessed results presented. fig. depicts gridsearch conducted svr kernel parameters given ranges. different measurements assess capability predictive models. order maintain consistency published results perform consistent comparison following measures coefficient determination spearman rank correlation coefficient expressed exp prd exp exp exp prd exp prd expected predicted values peptide binding affinity respectively number peptides exp mean expected values data set. measure statistical model based proportion variability data set 43. close suggests model successfully constructed. negative values indicate model poorly approximates expected values. spearman rank correlation coefficient measure statistical dependence variables. value ranges showing perfect correlation end. measures calculated task training testing. metric assess performance predictable models tasks fourth task assessed competition. results experiments carried discussed subsections. robustness proposed hybrid tsksvr method demonstrated data sets. second svr tsksvr compared order demonstrate performance fuzzy concept. present outcome feature selection methods showing amino acid locations amino acid scales. section results proposed model support vector based fuzzy system presented. test performance proposed model peptide data sets obtained coepra competition used. proposed approach takes account predictive problem large number attributes simulated practical data sets small number features consist noisefree samples. data sets contain 5000 descriptors peptide. difficulty analysis postgenome data curse dimensionality. curse dimensionality term usually related significant challenges occur working highdimensional data sets 45. small sample size important characteristics peptide data sets. consequence highdimensional nature data negatively effects performance prediction methods proposed approach exceptions. thousands features available peptides feature selection process integrated proposed model initial step obtain low dimensional feature space. mcfs able deal large number attributes peptide data sets efficiently reduced feature subset input variables rulebased fuzzy system. fcm study construct fuzzy rulebase. centroids clusters corresponding standard deviations obtained fcm design gaussian membership functions fuzzy models. rulebase fuzzy systems study tsk fuzzy system driven clustering methods cluster generally represents fuzzy rule. number clusters equivalent number rules fuzzy system. determining optimum number clusters consequently number rules clustering methods generally achieved considering outcome experimental studies different number clusters explored cluster structure yields best outcome e.g. minimum error regarded best set clusters. following concept studied number clusters noted cluster centres membership matrix randomly initialised fuzzy clustering stage. random initialisation fcm effect performance. number rules clusters experiments carried find optimum values parameters tsksvr model rule structure demonstrated figs. addition common problem support vector based approach easy determine kernel function 46. study svr trained linear kernel learn parameters consequent fuzzy model. parameters required optimised. optimisation svr parameters achieved gridsearch thousands values parameters tested rule base order find best set values parameters rule base yields highest tasks task. gridsearch repeated feature selection step end process best model selected. task graphs fluctuations reach local maximums particularly 100 features. rose gradually reach global maximum 161 features. reaching global maximum steady. task graphs increase gradually number features selected grew. reach local maximums features reach global maximum 247 features 246 features fig. task slight fluctuations observed graphs reaching local maximums 150 features reaching global maximum 165 features 172 features fig. task substantial fluctuations observed graphs reaching local maximums features reaching global maximum 141 features 121 features fig. example illustration fuzzy rules task rules provided fig. model 141 features possible fit paper features presented. rules read parameters membership function parameters finally consequent defined. rulebase proposed method able build robust interpretable fuzzy system highdimensional data set relatively small number data samples. observed optimum predictive model task obtained different sets rules presented table number rules needed smaller tasks task require rules obtain best possible outcome. rules task rules tasks yield best performance fuzzyrule base seven rules requirement optimum modelling task outcomes experiments clearly highlighted strengths tsksvr. fuzziness positively contributed modelling tasks. illustrate performance proposed hybrid method compared recently published results. coepra competition task contained participants. task contained participants. shown table results outperform competition results participant competed best model e.g. svr pls 29. addition task results obtained comparatively better recent studies presented 112912 13. compared best model presented literature predictive performance tasks improved 0.7 11.2 33.6 9.7 respectively. overall improvement gain tasks found 13.8. number studies present prediction peptide binding affinity svrbased analysis. tsksvr hybrid method combines svr fuzzyrule base tsk study important compare performances svr fuzzy concept. detailed table clear evidence tasks based recent literature svr prediction data sets training test cases proposed tsksvr algorithm outperforms solo version yields improvement 2.1 16.3 33.6 13.8 tasks respectively. outcome demonstrates superiority proposed hybrid approach mapping input output challenging highdimensional regression problem. optimal parameters tsksvr peptide binding affinity tasks found 2.40 0.05 rule size seven task 1.90 0.10 rule size task 1.45 0.90 rule size task 2.30 0.45 rule size task tsksvr models contained 161 247 172 141 features peptide binding affinity task respectively. worth noting approach tsksvr benefited svrbased training handled uncertainties peptide binding data set fuzzy modelling. svrbased experiments carried different peptide affinity data sets. rulebase rules range seven feature selection 250 features conducted reduce number features. amino acid features contributed efficiency proposed models given tables task amino acid features contributed output separate locations. amino acid feature numbered 481 hydrophobicity coefficient reversed phase high performance liquid chromatography contributed highest represented seven separate locations nonapeptide data set. task amino acid features contributed output separate locations. amino acid feature numbered 364 zimmbragg parameter sigma1.0e4 contributed highest represented seven separate locations octapeptide data set. task nineteen amino acid features contributed output separate locations. amino acid features numbered 110 composition 338 relative preference value 376 relative population conformational state 405 normalized positional residue frequency helix termini contributed highest represented separate locations nonapeptide data set. task amino acid features contributed output separate locations. amino acid features numbered 306 average relative fractional occurrence a0i 338 relative preference value contributed highest represented separate locations nonapeptide data set. amino acid feature numbered 400 polarity appeared task task task common feature location occurrences respectively. polarity amino acid considered highly discriminating feature data sets. results appear suggest different sets amino acid descriptors effect result exploration feature selection methods help accelerate predictive power proposed hybrid method. conclusions paper hybrid system tsksvr helped improve predictive ability tskfs significantly aid supportbased vector method developed demonstrated successful applications prediction peptide binding affinity regarded difficult modelling problems bioinformatics. far algorithmic approach concerned important conclusions driven svr enhanced adding fuzziness concept. tskfs benefited svrbased training. predictive performances improved compared best performance presented literature. overall improvement gain tasks found 13.8. apart improving prediction accuracy research study identified amino acid features polarity hydrophobicity coefficient zimmbragg parameter highly discriminating features peptide binding affinity data sets. amino acid features potentially considered better design peptides appropriate binding affinity. developed hybrid framework nonlinear system modelling based tsk fuzzy model consequent formed set linear equations. support vectors svr help form consequent model extended type2 fuzzy system closedform type reduction defuzzification method biglarbegianmelekmendel bmm based type2 fuzzy system explored example 4748. similarly concept generalised explore typen fuzzy system defuzzification phase performed approach. research carried direction. acknowledgements study volkan uslan funded montfort university leicester phd tuition fee scholarship. authors thank ovidiu ivanciuc organising coepra contest provided peptide binding affinity data sets. authors thank ozgurdemir kavuk assistance providing binding affinities test data sets. references',\n",
              " 'networks exhibit smallworld properties. structure smallworld network characterized short average path lengths high clustering coefficients. graph layout methods capture structure limits effectiveness utility visualization itself. present extension novel graphtpp layout method laying smallworld networks topological properties node attributes. wattsstrogatz model generate variety graphs smallworld network structure. community detection algorithms generate different clusterings data. clusterings adjacency matrix edgelist loaded graphtpp user interaction combined linear projections adjacency matrix graphtpp able produce layout visually separates clusters. layouts compared layouts forcebased techniques. graphtpp able clearly separate communities spatially distinct area edge relationships clusters strength relationship. secondary contribution edgegrouping algorithm graphtpp demonstrated means reduce visual clutter layout reinforce display strength relationship communities. introduction smallworld networks commonly occurring graph structure characterized short average path lengths high clustering coefficients means network large steps pair nodes. despite prevalence methods able lay structure communicated optimally examples networks display characteristics realworld include social networks biological networks geophysical ones high clustering coefficient smallworld networks provides interesting problem layout particularly shown users seek lay graphs clustered structure apparent ensuring clustered structure graph represented layout crucial enhancing users understanding graph context clusters relationships them. van ham van wijk proposed smallworld network specific layout methods gibson faith forward method based nodeattribute data. clusters represented graph extremely important human perceptual system naturally cause users assume relationship nodes placed close graphs highly clustered adhering principle laying graph key communicating structure network. generally forcedirected layouts produce accurate representation smallworld network structures. try optimize layout uniform edge lengths longer edge lengths usually required separate clusters forcedirected methods linlog linearlogarithmic 210 openord successor vxord try optimize clustering based topology methods use attributes precomputed clusterings. muelder mas treemap spacefilling approaches use precomputed clustering groupinthebox layout user input precomputed clustering. graphtpp graph targeted projection pursuit method encapsulated interactive software program previously layout smallworld networks nodeattributes. graph laid assisted direct user interaction according specified clustering resulting clear visual separation clusters. propose nodeattributes adjacency matrix graph replace multidimensional matrix attributes significantly increasing size graph terms number nodes demonstrates graphtpp scalable small examples previous work. paper proceeds follows. section2 discusses related work layout smallworld networks. section3 introduces wattsstrogatz model computing smallworld networks community detection algorithms layout. section4 presents graphtpp layouts smallworld networks twodimensional wattsstrogatz models whilst comparing results obtained openord forceatlas layout algorithms. introduces edgegrouping technique reducing visual clutter caused edges graphtpp layout. section5 discusses results limitations work recommending directions future work. section6 concludes paper. research shows graphtpp outperforms openord forceatlas method laying smallworld networks aim optimize layout communities detected community detection algorithms. main contribution demonstration graphtpp viable layout method typical nodeattribute data available base layout. related work smallworld networks characterized short average path lengths shortest path pair nodes high clustering coefficients e.g. social networks number friends person friends i.e. complete triangle. average path length grows logarithmically number nodes cliques near cliques. notable examples smallworld network milgrams degrees separation experiment proposed people united states time separated people chain friendship. recently boldi vigna modelled degree separation facebook graph 3.74. multiple examples smallworld networks occurring realworld including social networks biological networks geophysical networks albert barabsi hypothesized prevalence networks biology smallworld properties inherent structural advantages. number ways model smallworld network popular wattsstrogatz model wattsstrogatz model requires construction regular ring lattice followed random rewiring edges according rewiring probability produces graph short average path lengths high clustering coefficient compared realworld smallworld graphs degree distribution tend scalefree i.e. follow power law distribution. barabsialbert model produce scalefree network exhibit clustering properties integral smallworld networks essential visualization technique. given smallworld networks commonly occurring graph structure surprising little attention paid visualization. structural properties suited general forcebased layout methods methods try map shortest path lengths euclidean distances. short average path lengths possessed smallworld networks result high degree nodes placed close centre graph encourages wellknown hairballstyle layout form example paper fig. 9c. forcebased techniques try optimize layout according certain aesthetic criteria. criterion uniformity edge length long edge lengths required separate clusters forcestyle layouts represent clusters formed smallworld network years passed auber al. commented structural properties smallworld networks fully exploited visualization perspective knowledge techniques exist deal specifically layout graphs exhibiting smallworld properties. auber al.s multiscale smallworld layout solution. requires decomposing graph hierarchical manner rating strength edge removing socalled weaker ones. helps exploring individual clusters. smallworld network layouts include van ham van wijks uses adapted version noacks forcemodel separate clusters van ham wattenberg create sparse backbone graph removing edges low betweenness centrality laying graph level proceeding add edges in. topolayout detects smallworld features himap adapted kamada kawais algorithm produce clustered layouts. recently gibson faith projection based technique lay smallworld networks based nodeattributes aiming produce clustered layout. extend technique larger class smallworld networks longer rely nodeattributes instead use topological structure graph itself. hundreds solutions graph layout problem proposing different method highlight different features graph. forcebased techniques extremely popular generally suitable smallworld networks reasons detailed above. forcebased methods try optimize clustering potential candidates layout smallworld networks. noacks 210 linlog layouts example applied layout smallworld networks. linlog technique ignores uniform edge length criterion order separate clusters. derives fact model linear attractive forces logarithmic repulsive forces. noack shown method better separate graph clusters compared fruchtermanreingold method 23. openord forcebased method aims encourage clustering based simulated annealing algorithm. multilevel method cuts long edges reduce domination repulsive forces improves clustering. highly scalable algorithm lay graphs hundreds thousands nodes. similar forcebased methods use dimension reduction. classical distance multidimensional scaling mds use dissimilarity matrix shortest paths node pairs generate embedding pivotmds sampling approximation technique classical scaling nodes positioned according positions subset pivot nodes. highdimensional embedding hde similar pivotmds final step uses principal component analysis project layout space. methods use dimension reduction edgemaps relies nodeattributes mds projection layout pexgraph use attributes graphs connectivity layout. smallworld networks inherently clustered clusterbased layout techniques appropriate. methods groupinabox require clustering predefined place cluster bounded box nodes box cluster layout applied. muelder proposed treemap spacefilling approaches use community detection algorithms precompute clustering use communities determine decomposition treemap order placement spacefilling method. targeted projection pursuit tpp 2829 opensource exploratory visual interactive dimension reduction technique incorporated piece software known tpp tool provides gui frontend presents realtime visualization current projection ability user interact projection directly. httpscode.google.comptargetedprojectionpursuit. tool built data mining software weka tpp tool user able explore space possible linear projections highdimensional space dimensions. technique focuses areas finding projection groups points specified clusters identifying discriminatory dimensions describe analyze clusters thirdly identifying outliers. interactive user interface user separate nodes clusters drag twodimensional space fit intuition understanding data. underlying tpp algorithm search projection best matches target view user wants see. formally set entities described matrix defines entitys position kdimensional space. projection matrix maps entities dimensional space. user defines target view tpp searches projection minimizes difference target view projection. min projection matrix found training singlelayer perceptron artificial neural network inputs outputs. rows original matrix examined order standard backpropagation train network generate rows target matrix according leastsquares calculation. convergence reached original data transformed view connection weight input neuron output neurons gives weight dimension final projection projection matrix. graphtpp leverages tpp algorithm graph layout extends original tpp tool software import graph data updates visual display control panel facilitate viewing interaction graph. original motivation graphtpp use attributes nodes dimension reduction process creates layout. nodeattribute describe dimension data given clustering predefined kmeans algorithm aim lay graph clustered fashion layout direct result attributes bringing graphs topological structure attributes together. facilitate understanding graphs structure point view attributes. clustering particular identified important structural feature represented layout users favouring traditional aesthetic criteria use attributes layout create deeper understanding graph 31. graphtpp expressly aims separate graph clusters. use graphtpp based principle highdimensional data matrix describes features graphs nodes attribute matrix replaced adjacency matrix start section4. adjacency matrix associates node column row. entry position node connected node example fig. highlighted nodes edge shown graph circled table i.e. adjacency matrix. layout method common force techniques rely topological structure layout. graphtpp able leverage interactive features original tpp tool automatic separation points clusters direct user interaction number options adjusting visual features displayed layout node size colour shape etc.. graphtpp incorporates new options controlling visual features including edge shape appearance node labelling filtering options utilized graphs layouts paper. explanation process graphtpp paper given section4.1. smallworld networks generated according wattsstrogatz model implemented package igraph 32. wattsstrogatz model aims generate graph high clustering coefficient short average path length simulating characteristics smallworld network. basic wattsstrogatz smallworld model assumes ring lattice nodes connections node. edge randomly rewired probability describe regular network describes completely random network. graph structural properties describes path length describes clustering coefficient. value grows linearly clustered high high form largeworld network. random network poorly clustered low low grows logarithmically intermediate values small random random approximates smallworld properties short average path length high clustering coefficient. igraph package includes implementation wattsstrogatz model executed varying following parameters watts.strogatz.gamedim size neighbours dim dimension lattice ring lattice size number nodes dimension neighbours num. nearest neighbour connections rewiring probability. table shows parameters varied. constant rewiring probability 0.05 twodimensional models considered. network science community detection partitioning graph clusters communities. generally partitions divided node likely connected nodes community outside community. community detection usually based topological properties graph attributes. algorithms impose limit number communities graph partitioned place restrictions number. basic level identifying communities graph helps understanding graphs structure enable classification nodes structural position nodes periphery cluster potentially overlapping clusters. communities communicate hierarchical organization graph 34. paper use different community detection algorithms included igraph package. edgebetweenness hierarchical decomposition based iteratively removing edges lowest edgebetweenness centrality fastgreedy merges nodes communities greedy manner based optimizing modularity function leading eigenvector iteratively divides graph communities based signs eigenvector corresponds largest eigenvalue modularity matrix walktrap merges communities based short random walks staying cluster likely leaving spinglass statistical physics approach node takes spin states states changes depending state neighbouring nodes label propagation node assigned labels nodes common label neighbours. explanations community detection algorithms given supplemental material detailed review found fortunato 34. gibson faith demonstrated graphtpps capabilities layout smallworld networks. case network small nodes clustered groups. indicated potential graphtpp layout method smallworld networks. extend work applying layout larger networks. importantly artificially generated networks attributes graphs adjacency matrix instead. means like forcebased methods layout relies topological structure graph. compared forcebased techniques implemented graph layout software gephi martin al.s openord layout method forceatlas 15. forceatlas layout included comparison general nonoptimized clustering forcebased layout performance better fruchtermanreingold algorithm worse linlog terms clustering. system unavailability possible compare performance van ham van wijks framework networks generated wattsstrogatz model igraph package clusterings computed community detection algorithms recorded table graph created result running particular instance wattsstrogatz method. random rewiring probability run method parameters produce slightly different graph terms nodes connected difference affect validity results graph smallworld structure. community detection algorithms executed default parameters spinglass method run maximum spin states limited number communities ten. ensure algorithm generating cluster set excessively large graphtpp layout produced. community detection algorithms edgebetweenness fastgreedy leading eigenvector walktrap algorithms consistently produce community run graph terms number size communities created spinglass label propagation algorithms produce slightly different results time. differences large considered testing particular example algorithms sufficient demonstration purposes. graphs created node assigned community community detection algorithms command get.adjacency create adjacency matrix graph additional columns added identifying community node belonged community detection algorithm. exported csv ready converted arff file format supported weka. files simple list nodes communities belong edge list exported. arff file created loading csv file weka ensuring columns number nodes described numeric columns define dimensions projection. columns classtype columns define membership nodes particular communities community detection algorithm final column string column representing nodes id. file exported weka arff file imported graphtpp. edge list file imported graphtpp able represent edges graph. arff edge list file imported graphtpp user select particular set communities try separate nodes. following cases community detection algorithm selected separate points button pressed held begin automatic separation communities. action chosen graphtpp algorithm identifies number points space based number communities separate points maximally separated another. target projection automatic layout progresses trying achieve maximal separation. user believes automatic separation achieving desired level separation select set nodes individually community rectangle selection. selected user attempt node order achieve closer desired target view. fig. shows graph layout appear graphtpp. figures presented paper based svg file exported directly graphtpp. code data files needed recreate following steps found httpsgithub.comhelengibsongraphtpp. forceatlas layout openord layouts produced loading nodes file community attributes edge file graph layout software gephi running method default parameters exporting svg gephis export tool. section explores layouts produced layouts number different sized graphs community detection algorithms. network produced onedimensional model contained 500 nodes 5000 edges. 5000 edges produced requiring node connect nearest neighbours smallworld graph produced rewiring probability 0.05. community detection algorithm run graph set clusters produced algorithm. algorithms produced cluster sets varying sizes ranging 21. walktrap community detection algorithm based random walks partitioned graph clusters. fig. shows graph walktrap community detection algorithm applied layout methods. table supplemental material shows layout community detection algorithms graph. layout clearly influenced original ring structure network generated. graphtpp able separate clusters correctly distinct area. forceatlas separates clusters correctly snakelike layout means lead another. makes analysis cluster connected difficult. colour required determine cluster starts ends. openord method shows clusters leading nodes positioned tight groups. algorithm actually subdivides smaller clusters detected walktrap algorithm. clear colouring cluster subclusters belongs. seeing clusters stimulate investigation subdivided method particular significance clusters overlap. graphtpp tool layout facilitates analysis graph according attribute composition. fig. shows contribution different nodes layout. fig. red nodes possessing particular attribute case means nodes connected particular node. highlighted attributes clearly main contributors specific cluster i.e. node connects nodes cluster. attributes highlighted fig. nodes connect equally adjacent clusters. indicates nodes bridges communities. graph 500 nodes significant increase graph gibson faith fairly small. sixfold increase number nodes marked impact ability separate nodes clearly defined clusters forceatlas openord algorithms shown fig. 3000 nodes 15000 edges graphtpp able separate clusters clearly. case clusters determined walktrap algorithm. clear separation enables analysis strength relationships different communities evidenced fig. graph difficult identify nodes contribute determination cluster membership. average degree node cluster contains average 300 nodes. affect clustering ability graphtpp algorithm though. graph fig. volume edges graphtpp layout overwhelms overall layout graph clustered structure. means nodes clustered sheer number edges impedes ability interpret strength relationships clusters. section4.3 discusses solution problem extension graphtpp tool. 30004000 nodes difficult generate graphs calculate communities. large number attributes including node attribute brings led slow system performance running graphtpp typical desktop computer meant impractical attempt lay graphs 5000 nodes above. course increased processing power mitigate effect. future work focus carrying detailed performance analysis graphtpp algorithm look opportunities code optimization. type graph structure 3000 nodes current practical limit graphtpp operating regular desktop environment. layouts graphs 1000 1500 2000 nodes found supplementary material. experimenting layout method quickly clear volume edges displayed impacting graphtpps efficacy. example fig. clusters separated number edges makes difficult interpret strongly cluster connected. layout successful positioning nodes dealing edges problem distracting interesting structures display. edgebundling algorithms group edges reduce visual clutter. multiple edgebundling algorithms proposed previously 4143 use edgegrouping method makes use intrinsic properties graphtpp layout clusters. centroid cluster found edge drawn node centroid cluster exact position offset slightly centroid order allow thickness edgegroup proportional number edges contains. bundle reaches centroid cluster edges split edgegroup complete edge shown fig. unlike fig. actual graphtpp layout usually nodes placed centroid position artefact edges meet edgegroup hidden. bundled view intended improve overview graph individual edges explored. fig. shows edgegrouped layout corresponds layout fig. 5a. visual clutter significantly reduced easier relationship strengths clusters. slightly smaller example 2000 nodes 10000 edges fewer clusters compared clarity edgegroups offer obvious. fig. shows graphtpp layout graph original edges grouped edges sidebyside. fig. difficult distinguish volume edges clusters differs depending pair clusters chosen fig. clearer. example edgegroups linked cluster fig. appear thicker belonging clusters. table shows total number edges cluster 2000 node graph supports conclusion values cluster highest second highest number connections clusters. edgegrouped version graphtpp able clusters high connectivity immediately original graphtpp version cluster nodes expressly distinguish connected were. fig. shows 2000 node graph laid nodexl groupinabox layout main layout harel korens fast multiscale layout box layouts. method rigidly adheres clustered structure constraining cluster box. fig. uses edge bundling algorithm display edges fig. uses combined edge method account number edges cluster. terms understanding relationships clusters layout able graphtpp layout fig. able withincluster structure. twodimensional model incrementally increases number nodes dimension. example size gives 400 nodes 2020. test models maximum 1600 nodes 4040. model setting nearest neighbours parameter balanced requirement having edges attributes overloading graph. initial 400 node model 4800 edges community detection algorithms divided graph sets clusters reasonable sizes algorithm algorithms yielded communities respectively corresponding row table leading eigenvector community detection algorithm fig. shows forceatlas layout struggling produce described hairball layout openord algorithm shows clear structure correspond community detection algorithms case table supplemental material comparisons. graphtpp able cluster nodes communities bundling edges brings structure clearly preventing edges overpowering graph. graphs produced twodimensional model lower limit maximum number nodes graphtpp lay compared onedimensional model. case 1225 nodes 7350 edges maximum. seen fig. displays clustering found edgebetweeness algorithm forceatlas unable produce layout clearly represented structure graph. openord algorithm able structure divides nodes different communities. graphtpp displays layout clearly respects communities detected edgebetweenness algorithm. particularly useful aim understand relationships different communities thicker bundle greater number edges communities. examples layouts shown supplemental material graphs 400 625 900 nodes. section shown graphtpp able layout smallworld graphs respect community clustered structure way community structure communicated layout graph. demonstrated terms visual separation cluster graphtpp able produce type layout reliably consistently layout methods force atlas openord. laying smallworld network graphtpp allows view clustered structure graph visually clear visual separation better understand relationships different clusters. main aims graph layout better understand relationships different nodes graph visually. graphtpp provides ability interactive platform potential applied number different realworld smallworld networks generate insights graphs. forceatlas openord able produce layouts appeared aesthetically pleasing layouts able communicate clustered structure way graphtpp was. furthermore graphtpp limits terms size graph lay degradation layout quality lager graphs apparent forceatlas openord graphs graphtpp despite fact overall forceatlas openord able lay graphs larger size. discussion structure smallworld network perfectly suited visualization. representing communities form graph principal aim layout want visualization communicate structure effectively. original graphtpp relied largely nodeattributes drive layout methods highly clustered structure characterizes smallworld networks meant potential adapt layout method replacing nodeattribute matrix symmetric adjacency matrix graph. edges entries attribute matrix concentrated clusters graphtpp good candidate graph layout. paper shown smallworld networks generated wattsstrogatz model dimensions graphtpp successful terms visual presentation visual distinction clusters representation clusters clustering nodes appropriate predefined communities producing graph layout outperformed layout algorithms terms representing structure. forcebased methods initially clearly showed structure communities layouts graphs grew larger complex quality techniques degraded. smallest twodimensional models forceatlas method able produce layout kind structure openord method usually divided clusters divided completely differently. edgegrouped graphtpp layout enhanced appearance utility graphtpp layout removing visual clutter caused edges tendency overload representation. edgegroups provide advantage showing strongly cluster connected determine important clusters graph influence. despite advantages graphtpp layout described sections number limitations associated graphtpp method laying graphs. importing data graphtpp limit number clusters nodes divided attempt spatially separate limitation especially smallworld structure clusters begin equally spaced display space. results graphtpp losing power able structure graph. goals tpp attributebased graphtpp identify common attributes occur cluster. typical uses tpp attributes emerge significant attributes define cluster form useful analysis. limitation case nodes attributes edges attribute values set significant attributes cluster larger. node connects nodes attribute nonzero values. terms analysis means cluster analyzed significant attributes likely nodes contained cluster telling little new graph attribute analysis perspective. clustering nodes causes issue. seen table number cluster edges far higher clusters. good thing sense makes graphtpp layout algorithm work type graph tight clustering comes expense able cluster edges instead obscured nodes. problem limited graphtpp occur layout clusters nodes closely together. groupinabox layout showed opposite problem cluster structure clearer relationships clusters so. difficulties representing smallworld graphs particular edges community usually connected community. means edgegrouped layout lot displayed edges graph look cluttered disorganized. effect bundled layout actually layout small clique cluster represented single node connected node. problem exacerbated number communities increases. advantage graphtpp layout maintains layouts proximity clusters related topology analyzed nodes contribution. potential solution filter edges based bundle size edges bundle certain size. limitation computation volume attributes graphtpp able calculations layout interactive interaction longer happens realtime gap user instructing nodes actually move. tend jump screen smoothly. diminishes interaction experience graphtpp intermediate stages interactive exploration. mentioned previous section ability filter weaker bundled edges aid clarity usefulness layout. edges projection displayed. investigation methods matrix diagrams way density cluster edges aid visualization method group nodes clusters suffer occlusion cluster edges. allow user investigate subpatterns cluster particular features apparent nodes tightly clustered together. determining specific set tasks easily accomplished type layout provide advantage. particular kind layout support overview based tasks case mean user know lay graph graphtpp technique specific query line investigation mind. paper shown assessing strength connectivity clusters potential task. conclusions paper presented extension smallworlds pilot study presented gibson faith graphtpp lay smallworld network node attributes. case number nodes significantly increased graphtpp shows superior ability produce layout reflects clustered structure graph compared forcebased methods. addition edge bundling method means strength association communities clearly visible. use community detection algorithms resulted reasonably equally sized clusters aided analysis visualization. showed smallworld networks compatible repurposing edges attribute values graphtpp theory applied viable layout option graph regardless existence preexisting attributes not. detailed description community detection algorithms found supplementary material. examples layouts graph different sizes clustered community detection algorithms detailed table found supplementary material. supplementary data associated article found online version httpdx.doi.org10.1016j.asoc.2016.01.036. following supplementary data article references']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5PGcyem1fYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "3a641572-1532-4380-f04e-f81892dbb7e5"
      },
      "source": [
        "# Preprocessing \"highlight\" text\n",
        "\n",
        "\n",
        "def clean_highlight(text):\n",
        "  newText = text.lower()\n",
        "  newText = re.sub('[^\\w\\s\\d\\.]','',newText)\n",
        "  newText = ' '.join(newText.split())\n",
        "  newText = '_START_ '+ newText + ' _END_'\n",
        "  return newText\n",
        "\n",
        "cleaned_highlight = []\n",
        "for t in data['highlights']:\n",
        "    cleaned_highlight.append(clean_highlight(t))\n",
        "\n",
        "cleaned_highlight[:5]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_START_ we develop an evolutionary approach to solve interrelated optimisation problems. multiple agents autonomously deal with their own problems and react to the others. test problems in water pollution and aerospace modelling demonstrate the algorithm. experiments on scalability and convergence of the algorithm show promising results. _END_',\n",
              " '_START_ the aggregated artificial neural network was used to investigate the simultaneous effects of printing parameters on the compressive strength and porosity of scaffolds. particle swarm optimization algorithm was implemented to obtain the optimum topology of the aann. pareto front optimization was used to determine the optimal setting parameters. the presented results and discussion can give informative information to practitioners who want to design a porous structure and need to know the impact of influential design parameters. _END_',\n",
              " '_START_ a stochastic global optimization framework for open pit mining complexes is proposed. the method simultaneously optimizes production schedules and downstream processes. the modeling is flexible and may be applied to numerous types of mining complexes. three combinations of metaheuristics are tested. results from an example show a substantial economic benefit when using this approach. _END_',\n",
              " '_START_ highdimensional biological data sets are modelled with a regression based fuzzy system. an svr based fuzzy model is proposed to find degree of peptide binding to mhc molecules. svr is enhanced by adding the fuzziness concept. tsk fuzzy system is benefited from svrbased training. the proposed models suggest that the predictive ability and performance are increased. _END_',\n",
              " '_START_ few graph layout methods capture the community structure of small world networks. we present graphtpp an attribute and dimension reduction based layout method. graphtpp is able to separate communities into spatially distinct areas. separation is better than other clustering based layouts. edgegrouping between communities shows the strength of their relationships. _END_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrCEGfX29OAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Storing preprocessed data in the dataframe\n",
        "\n",
        "\n",
        "data['cleaned_highlights'] = cleaned_highlight\n",
        "data['cleaned_body'] = cleaned_body"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoU_mCHouOx-",
        "colab_type": "text"
      },
      "source": [
        "Retriving 5000 papers from the dataframe as more computational power is needed for the entire 8k data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXzYQJsExMA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yso-fdsWxYDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "78944a37-0729-4c7e-ad92-c71ee9ed4831"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5000 entries, 27 to 6950\n",
            "Data columns (total 6 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   text                5000 non-null   object\n",
            " 1   filenames           5000 non-null   object\n",
            " 2   highlights          5000 non-null   object\n",
            " 3   body                5000 non-null   object\n",
            " 4   cleaned_highlights  5000 non-null   object\n",
            " 5   cleaned_body        5000 non-null   object\n",
            "dtypes: object(6)\n",
            "memory usage: 273.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q1XQiGXucxP",
        "colab_type": "text"
      },
      "source": [
        "Visualizing the document length distribution for \"body\" and \"highlights\" to estimate optimal maximum length to create equal length vectors for words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S7LvNebUubSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "7dbd09f1-fbda-493e-e224-6540c014eb1f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "for i in data['cleaned_body']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_highlights']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'Body':text_word_count, 'Highlights':summary_word_count})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f93_mXH2_LXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_body = 1000\n",
        "max_len_highlight = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlkcLDYHmhAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2977eea8-aff5-4406-b798-bcef88450f3f"
      },
      "source": [
        "# Converting cleaned data into strings\n",
        "\n",
        "\n",
        "data.cleaned_body = data.cleaned_body.progress_apply(lambda x: str(x))\n",
        "data.cleaned_highlights = data.cleaned_highlights.progress_apply(lambda x: str(x))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [00:00<00:00, 413231.92it/s]\n",
            "100%|██████████| 5000/5000 [00:00<00:00, 591580.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9f-RGEriHLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting data into training and test sets\n",
        "# Test set is 20% of total data\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(data['cleaned_body'],data['cleaned_highlights'],test_size=0.2,random_state=0,shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2_3_c3vdZt",
        "colab_type": "text"
      },
      "source": [
        "Tokenizer class from Keras is used to create equal length vectors for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpHCxYUbjJPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing \"body\"\n",
        "x_tok = Tokenizer()\n",
        "x_tok.fit_on_texts(list(x_train))\n",
        "\n",
        "# Converting text to number sequences\n",
        "x_train = x_tok.texts_to_sequences(x_train) \n",
        "x_test = x_tok.texts_to_sequences(x_test)\n",
        "\n",
        "# Padding zero upto maximum length\n",
        "x_train = pad_sequences(x_train,  maxlen=max_len_body, padding='post') \n",
        "x_test = pad_sequences(x_test, maxlen=max_len_body, padding='post')\n",
        "\n",
        "# Total number of words\n",
        "x_vocab_size = len(x_tok.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F62DsNW8lIaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing \"highlights\"\n",
        "y_tok = Tokenizer()\n",
        "y_tok.fit_on_texts(list(y_train))\n",
        "\n",
        "# Converting text to number sequences\n",
        "y_train = y_tok.texts_to_sequences(y_train) \n",
        "y_test = y_tok.texts_to_sequences(y_test)\n",
        "\n",
        "# Padding zero upto maximum length\n",
        "y_train = pad_sequences(y_train,  maxlen=max_len_highlight, padding='post') \n",
        "y_test = pad_sequences(y_test, maxlen=max_len_highlight, padding='post')\n",
        "\n",
        "# Word count\n",
        "y_vocab_size = len(y_tok.word_index) +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk7dNvoUsN_S",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q93pUkCJsUge",
        "colab_type": "text"
      },
      "source": [
        "Attention Layer required to remember context information. As there is no Keras implementation for Attention Layer we have found a third-party resource to aid in our model. Reference: https://github.com/thushv89/attention_keras/blob/master/layers/attention.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZzO2tHAn4TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEnxdZFbsZJV",
        "colab_type": "text"
      },
      "source": [
        "Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gekeC5WIl4U5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "ed55bed6-b583-41ea-d90a-5cff3b9d4bdc"
      },
      "source": [
        "#from keras import backend as K \n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "K.clear_session() \n",
        "latent_dim = 50 \n",
        "\n",
        "\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_len_body,)) \n",
        "enc_emb = Embedding(x_vocab_size, latent_dim,trainable=True)(encoder_inputs) \n",
        "\n",
        "# 1st LSTM Layer\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "# 2nd LSTM Layer\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "# 3rd LSTM Layer\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "\n",
        "# Decoder \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(y_vocab_size, latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "# LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "# Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input) \n",
        "\n",
        "# Model Definition\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1000)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1000, 50)     12730400    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 1000, 50), ( 20200       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 1000, 50), ( 20200       lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 50)     819550      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 1000, 50), ( 20200       lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 50), ( 20200       embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 50), ( 5050        lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 100)    0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 16391)  1655491     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 15,291,291\n",
            "Trainable params: 15,291,291\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBQpEFfwjmb",
        "colab_type": "text"
      },
      "source": [
        "RMSProp was found to be the best optimizer for text summarization and sparse categorical crossentropy was found to be the best loss functiom from this reference: https://hackernoon.com/text-summarization-using-keras-models-366b002408d9\n",
        "\n",
        "Sparse categorical cross entropy is used as the loss function because it automatically converts an integer sequence to a one-hot encoded vector, hence requiring less memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb4o-pcMnmR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL253riQxSi7",
        "colab_type": "text"
      },
      "source": [
        "Validation loss was monitored to make sure that the model does not get worse. The model training is stopped if validation loss for the current iteration is higher than the validation loss for the previous iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbPRkDhaJfvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-zxfc2Use98",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELKGfGSSJ-jO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "4731f5d4-abb0-4080-f9bc-dc616fe079df"
      },
      "source": [
        "history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:], epochs=30, callbacks=[es], batch_size=128, validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-136502d92dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2772\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 2774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[1;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 2706\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    <ipython-input-21-f77bd7cfbe90>:106 call  *\n        last_out, e_outputs, _ = K.rnn(\n    /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3104 rnn  *\n        reachable = tf_utils.get_reachable_from_inputs([learning_phase()],\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py:140 get_reachable_from_inputs  **\n        raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\n\n    TypeError: Expected Operation, Variable, or Tensor, got 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fizr8YPC-QZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/NLP Project/Project Final/model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogiyb96NK0zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing training ans test loss functions\n",
        "\n",
        "\n",
        "from matplotlib import pyplot \n",
        "pyplot.plot(history.history['loss'], label='train') \n",
        "pyplot.plot(history.history['val_loss'], label='test') \n",
        "pyplot.legend() \n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg_mIo16ytnB",
        "colab_type": "text"
      },
      "source": [
        "Building dictionary for target and source words, to convert the index to word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gEYRwn-A3i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tok.index_word \n",
        "reverse_source_word_index=x_tok.index_word \n",
        "target_word_index=y_tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys7Rrz2tsinX",
        "colab_type": "text"
      },
      "source": [
        "## Inference Stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOhWVQw4APQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encoder Inference\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder Inference\n",
        "# Below tensors hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_body,latent_dim))\n",
        "\n",
        "# Getting decoder sequence embeddings\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Predicting the next word in the sequence\n",
        "# Setting the initial states to the previous time step states\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# Attention Inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# Dense softmax layer to calculate probability distribution over target vocab\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final Decoder model\n",
        "decoder_model = Model(\n",
        "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "[decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QEq35cBAPIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to implement inference\n",
        "\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encoding input as state vectors\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generating empty target sequence of length 1\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Taking the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = target_word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        try:\n",
        "            sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        except:\n",
        "            sampled_token = reverse_target_word_index[np.random.randint(1, len(reverse_target_word_index))]\n",
        "        if(sampled_token!='end'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_len_highlight-1)):\n",
        "                stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pTxOJBc0lb2",
        "colab_type": "text"
      },
      "source": [
        "Function to convert interger sequence to word sequence for highlights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu6LNrvFAO8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2highlights(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RscqnZlD1BoC",
        "colab_type": "text"
      },
      "source": [
        "## Displaying Results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFwhBovZAjic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reference = []\n",
        "hypothesis = []\n",
        "for i in range(10):\n",
        "  print(\"Highlights:\")\n",
        "  print(seq2summary(y_test[i]))\n",
        "  reference.append(seq2highlights(y_test[i]))\n",
        "  print(\"\\n\")\n",
        "  print(\"Predicted summary:\")\n",
        "  print(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
        "  hypothesis.append(decode_sequence(x_test[i].reshape(1,max_len_body)))\n",
        "  print(\"\\n\")\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZktzSNvkjPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np9ldkA5tX_7",
        "colab_type": "text"
      },
      "source": [
        "## Calculating Rouge Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa_-zaknr3C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXfbHFbhqzvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = Rouge()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCtxBD7rqzsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score.get_scores(hypothesis, reference, avg = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jeQkspBrJhy",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "ROUGH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBF76h7drbUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading papers, cleaning, creating & pickling dataframe\n",
        "\n",
        "\n",
        "def read_paper(path):\n",
        "  \"\"\"\n",
        "  Reads research papers and store them in a string\n",
        "  \"\"\"\n",
        "  f = open(path, 'r', encoding=\"utf-8\")\n",
        "  text = str(f.read())\n",
        "  f.close()\n",
        "  return text\n",
        "\n",
        "\n",
        "def create_dataframe(NO_INPUT_PAPERS):\n",
        "  \"\"\"\n",
        "  Takes number of papers to read and stores data in a dataframe\n",
        "  \"\"\"\n",
        "  temp_papers = []\n",
        "  filenames = []\n",
        "  for filename in tqdm(glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")[:NO_INPUT_PAPERS]):\n",
        "      temp_papers.append(read_paper(filename))\n",
        "      filenames.append(filename)\n",
        "  return pd.DataFrame(list(zip(temp_papers, filenames)), columns =['text', 'filenames'])\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "  \"\"\"\n",
        "  Removes unwanted characters, accounting for unicode characters\n",
        "  \"\"\"\n",
        "  text = re.sub(\"@&#\", \" \", text)\n",
        "  text = re.sub(\"\\n\", \" \", text)\n",
        "  text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "  return text\n",
        "\n",
        "\n",
        "data = create_dataframe(NO_INPUT_PAPERS = 10000)\n",
        "\n",
        "data['highlights'] = data['text'].progress_apply(lambda x: re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', x,  flags = re.I)[0])\n",
        "data = data[data.highlights != '    ']\n",
        "\n",
        "data['body'] = data['text'].progress_apply(lambda x: re.findall(r'.*(?:abstract )(.*?)$', x,  flags = re.I)[0])\n",
        "\n",
        "data.to_pickle(\"./papers.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-upXdNQPGPh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_pickle(\"/content/drive/My Drive/NLP Project/Project Final/data.pkl\")\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eth5gY49rJOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to find filename which don't have \"highlights\" in it\n",
        "\n",
        "\n",
        "def file_name(NO_INPUT_PAPERS):\n",
        "  temp_papers = []\n",
        "  for filename in glob.glob(\"/content/drive/My Drive/NLP Project/Project Final/Parsed_Papers/*.txt\")[:NO_INPUT_PAPERS]:\n",
        "    text = read_paper(filename)\n",
        "    text = re.sub(\"@&#\", \" \", text)\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)\n",
        "    if highlights == []:\n",
        "      print(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYkIYLB7rIkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}